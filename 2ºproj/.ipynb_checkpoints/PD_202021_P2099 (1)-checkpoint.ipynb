{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining / Prospecção de Dados\n",
    "\n",
    "## Diogo F. Soares and Sara C. Madeira, 2020/21\n",
    "\n",
    "# Project 2 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics \n",
    "\n",
    "**_Read Carefully_**\n",
    "\n",
    "**Students should work in teams of 2 or 3 people**. \n",
    "\n",
    "Individual projects might be allowed (with valid justification), but will not have better grades for this reason. \n",
    "\n",
    "The quality of the project will dictate its grade, not the number of people working.\n",
    "\n",
    "**The project's solution should be uploaded in Moodle before the end of `April, 18th (23:59)`.** \n",
    "\n",
    "Students should **upload a `.zip` file** containing all the files necessary for project evaluation. \n",
    "Groups should be registered in [Moodle](https://moodle.ciencias.ulisboa.pt/mod/groupselect/view.php?id=139096) and the zip file should be identified as `PDnn.zip` where `nn` is the number of your group.\n",
    "\n",
    "**It is mandatory to produce a Jupyter notebook containing code and text/images/tables/etc describing the solution and the results. Projects not delivered in this format will not be graded. You can use `PD_202021_P2.ipynb`as template. In your `.zip` folder you should also include an HTML version of your notebook with all the outputs** (File > Download as > HTML).\n",
    "\n",
    "**Decisions should be justified and results should be critically discussed.** \n",
    "\n",
    "_Project solutions containing only code and outputs without discussions will achieve a maximum grade 10 out of 20._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Tools\n",
    "\n",
    "In this project you should use [Python 3](https://www.python.org), [Jupyter Notebook](http://jupyter.org) and **[Scikit-learn](http://scikit-learn.org/stable/)**.\n",
    "\n",
    "The dataset to be analysed is **`medulloblastoma_genes.csv`**. It includes 76 samples of medulloblastoma (MB) with respective expression levels of 54.675 genes measured in children with ages between 3 and 16 years. Medulloblastoma is a malignant childhood brain tumour comprising four discrete subgroups. \n",
    "\n",
    "In this project you will consider the labels of the samples included in the `labels.csv` file where samples are labelled as MB-CL or Other. In this case, we have 51 samples of classic medulloblastoma (MB-CL) and 25 other types (namely: 6 desmoplastic nodular, 17 anaplastic and 2 medullomyoblastoma).\n",
    "\n",
    "In `medulloblastoma_genes.csv` each line represents a sample and each column represents a gene.\n",
    "\n",
    "\n",
    "**The goal is to cluster samples and (ideally) find \"MB-CL\" groups and \"Other MB\" groups.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Identification\n",
    "\n",
    "**GROUP NNN**\n",
    "\n",
    "Students:\n",
    "\n",
    "* Student 1 - n_student1\n",
    "* Student 2 - n_student2\n",
    "* Student 3 - n_student3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this step you should have:\n",
    "* a 76 rows × 54675 columns matrix, **X**, containing the values of the 54675 features for each of the 76 samples.\n",
    "* a vector, **y**, with the 76 type of medulloblastoma, which you can use later to evaluate clustering quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset to be analysed is **`medulloblastoma_genes.csv`**. It includes 76 samples of medulloblastoma (MB) with respective expression levels of 54.675 genes measured in children with ages between 3 and 16 years. Medulloblastoma is a malignant childhood brain tumour comprising four discrete subgroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.read_csv(\"medulloblastoma_genes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.to_pickle(\"./medulloblastoma_genes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_pickle('medulloblastoma_genes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().sum().sum() #checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.duplicated().sum() #checking for duplicated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.rename(columns = {'Unnamed: 0': 'samples'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.samples.nunique() #there are 76 unique samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samples = X.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['samples'],axis=1, inplace=True) #removing the id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 54675)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1053_at</th>\n",
       "      <th>117_at</th>\n",
       "      <th>121_at</th>\n",
       "      <th>1255_g_at</th>\n",
       "      <th>1294_at</th>\n",
       "      <th>1316_at</th>\n",
       "      <th>1320_at</th>\n",
       "      <th>1405_i_at</th>\n",
       "      <th>1431_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-r2-Ec-bioD-3_at</th>\n",
       "      <th>AFFX-r2-Ec-bioD-5_at</th>\n",
       "      <th>AFFX-r2-P1-cre-3_at</th>\n",
       "      <th>AFFX-r2-P1-cre-5_at</th>\n",
       "      <th>AFFX-ThrX-3_at</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.59594</td>\n",
       "      <td>6.14440</td>\n",
       "      <td>4.82431</td>\n",
       "      <td>5.67092</td>\n",
       "      <td>4.14155</td>\n",
       "      <td>5.48935</td>\n",
       "      <td>6.01078</td>\n",
       "      <td>5.17671</td>\n",
       "      <td>4.83708</td>\n",
       "      <td>4.77996</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2364</td>\n",
       "      <td>10.1202</td>\n",
       "      <td>11.5560</td>\n",
       "      <td>11.2308</td>\n",
       "      <td>6.94803</td>\n",
       "      <td>6.42487</td>\n",
       "      <td>6.70991</td>\n",
       "      <td>4.48639</td>\n",
       "      <td>4.43082</td>\n",
       "      <td>3.48738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.80665</td>\n",
       "      <td>6.19154</td>\n",
       "      <td>5.24439</td>\n",
       "      <td>5.62149</td>\n",
       "      <td>3.41773</td>\n",
       "      <td>5.70478</td>\n",
       "      <td>5.98896</td>\n",
       "      <td>3.45316</td>\n",
       "      <td>4.34121</td>\n",
       "      <td>4.28772</td>\n",
       "      <td>...</td>\n",
       "      <td>10.4747</td>\n",
       "      <td>10.4893</td>\n",
       "      <td>11.6799</td>\n",
       "      <td>11.3475</td>\n",
       "      <td>6.09785</td>\n",
       "      <td>4.38328</td>\n",
       "      <td>4.68583</td>\n",
       "      <td>3.40120</td>\n",
       "      <td>4.20916</td>\n",
       "      <td>4.40915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 54675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1007_s_at  1053_at   117_at   121_at  1255_g_at  1294_at  1316_at  1320_at  \\\n",
       "0    7.59594  6.14440  4.82431  5.67092    4.14155  5.48935  6.01078  5.17671   \n",
       "1    7.80665  6.19154  5.24439  5.62149    3.41773  5.70478  5.98896  3.45316   \n",
       "\n",
       "   1405_i_at  1431_at  ...  AFFX-r2-Ec-bioD-3_at  AFFX-r2-Ec-bioD-5_at  \\\n",
       "0    4.83708  4.77996  ...               10.2364               10.1202   \n",
       "1    4.34121  4.28772  ...               10.4747               10.4893   \n",
       "\n",
       "   AFFX-r2-P1-cre-3_at  AFFX-r2-P1-cre-5_at  AFFX-ThrX-3_at  AFFX-ThrX-5_at  \\\n",
       "0              11.5560              11.2308         6.94803         6.42487   \n",
       "1              11.6799              11.3475         6.09785         4.38328   \n",
       "\n",
       "   AFFX-ThrX-M_at  AFFX-TrpnX-3_at  AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  \n",
       "0         6.70991          4.48639          4.43082          3.48738  \n",
       "1         4.68583          3.40120          4.20916          4.40915  \n",
       "\n",
       "[2 rows x 54675 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project you will consider the labels of the samples included in the `labels.csv` file where samples are labelled as MB-CL or Other. In this case, we have 51 samples of classic medulloblastoma (MB-CL) and 25 other types (namely: 6 desmoplastic nodular, 17 anaplastic and 2 medullomyoblastoma).\n",
    "\n",
    "In `medulloblastoma_genes.csv` each line represents a sample and each column represents a gene.\n",
    "\n",
    "\n",
    "**The goal is to cluster samples and (ideally) find \"MB-CL\" groups and \"Other MB\" groups.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSM918578</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GSM918579</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GSM918580</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSM918581</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GSM918582</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>GSM918649</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>GSM918650</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>GSM918651</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>GSM918652</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>GSM918653</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      samples  class\n",
       "0   GSM918578  MB-CL\n",
       "1   GSM918579  MB-CL\n",
       "2   GSM918580  MB-CL\n",
       "3   GSM918581  MB-CL\n",
       "4   GSM918582  Other\n",
       "..        ...    ...\n",
       "71  GSM918649  Other\n",
       "72  GSM918650  MB-CL\n",
       "73  GSM918651  Other\n",
       "74  GSM918652  MB-CL\n",
       "75  GSM918653  MB-CL\n",
       "\n",
       "[76 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(\"labels.csv\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_samples = y.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes =  76\n",
      "Output classes =  ['GSM918578' 'GSM918579' 'GSM918580' 'GSM918581' 'GSM918582' 'GSM918583'\n",
      " 'GSM918584' 'GSM918585' 'GSM918586' 'GSM918587' 'GSM918588' 'GSM918589'\n",
      " 'GSM918590' 'GSM918591' 'GSM918592' 'GSM918593' 'GSM918594' 'GSM918595'\n",
      " 'GSM918596' 'GSM918597' 'GSM918598' 'GSM918599' 'GSM918600' 'GSM918601'\n",
      " 'GSM918602' 'GSM918603' 'GSM918604' 'GSM918605' 'GSM918606' 'GSM918607'\n",
      " 'GSM918608' 'GSM918609' 'GSM918610' 'GSM918611' 'GSM918612' 'GSM918613'\n",
      " 'GSM918614' 'GSM918615' 'GSM918616' 'GSM918617' 'GSM918618' 'GSM918619'\n",
      " 'GSM918620' 'GSM918621' 'GSM918622' 'GSM918623' 'GSM918624' 'GSM918625'\n",
      " 'GSM918626' 'GSM918627' 'GSM918628' 'GSM918629' 'GSM918630' 'GSM918631'\n",
      " 'GSM918632' 'GSM918633' 'GSM918634' 'GSM918635' 'GSM918636' 'GSM918637'\n",
      " 'GSM918638' 'GSM918639' 'GSM918640' 'GSM918641' 'GSM918642' 'GSM918643'\n",
      " 'GSM918644' 'GSM918645' 'GSM918646' 'GSM918647' 'GSM918648' 'GSM918649'\n",
      " 'GSM918650' 'GSM918651' 'GSM918652' 'GSM918653']\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y_samples)\n",
    "n_classes = len(classes)\n",
    "print('Total number of classes = ', n_classes) #there are 76 unique values as we expect\n",
    "print('Output classes = ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "(X_samples == y_samples).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.drop(['samples'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  class\n",
       "0       0  MB-CL\n",
       "1       1  MB-CL\n",
       "2       2  MB-CL\n",
       "3       3  MB-CL\n",
       "4       4  Other\n",
       "..    ...    ...\n",
       "71     71  Other\n",
       "72     72  MB-CL\n",
       "73     73  Other\n",
       "74     74  MB-CL\n",
       "75     75  MB-CL\n",
       "\n",
       "[76 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.reset_index()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>MB-CL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class\n",
       "0   MB-CL\n",
       "1   MB-CL\n",
       "2   MB-CL\n",
       "3   MB-CL\n",
       "4   Other\n",
       "..    ...\n",
       "71  Other\n",
       "72  MB-CL\n",
       "73  Other\n",
       "74  MB-CL\n",
       "75  MB-CL\n",
       "\n",
       "[76 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you already noticed the number of features (genes) is extremely high when compared to the number of objects to cluster (samples). In this context, you should perform dimensionality reduction, that is, reduce the number of features, in two ways:\n",
    "\n",
    "* [**Removing features with low variance**](http://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "\n",
    "* [**Using Principal Component Analysis**](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "At the end of this step you should have two new matrices with the same number of rows, each with a different number of columns (features): **X_variance** and **X_PCA**. \n",
    "\n",
    "**Don't change X you will need it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_score, chi_2_p_value = chi2(X,y)\n",
    "f_score, f_p_value = f_classif(X,y)\n",
    "mut_info_score = mutual_info_classif(X,y)\n",
    "\n",
    "low_var_df=pd.DataFrame({\"names\": X.columns, \n",
    "                \"chi2\": chi2_score,\n",
    "                \"chi2_p\": chi_2_p_value,\n",
    "                \"F\": f_score,\n",
    "                \"F_p\": f_p_value,\n",
    "                \"MI\": mut_info_score})\n",
    "low_var_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.var().sort_values() #getting the different ranges of all the features of the original dataframe X\n",
    "# we need to normalize the features since they have such different ranges (the min is 0.003 and the max is  7.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = pd.DataFrame(scaler.fit_transform(X.values), columns=X.columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2.var().sort_values() #variance of the columns of X after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_var_pd = []\n",
    "for i in np.arange(X_2.var().sort_values().min(), X_2.var().sort_values().max(),0.01):\n",
    "    low_var_pd.append((np.round(i,2), \n",
    "                      X_2.loc[:, X_2.var(axis=0) > np.round(i,2)].shape[1], \n",
    "                      X_2.shape[1]-X_2.loc[:, X_2.var(axis=0) > np.round(i,2)].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_var_pd = pd.DataFrame(low_var_pd, columns=('Threshold', 'Columns Remaining', 'Columns Removed'))\n",
    "low_var_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_var_pd.plot.line(x='Threshold', y='Columns Removed', c='r')\n",
    "low_var_pd.plot.line(x='Threshold', y='Columns Remaining', c='g')\n",
    "#plots to help decide an appropriate threshold for the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_variance = X_2.loc[:, X_2.var(axis=0) > 0.04] \n",
    "#creating X_variance with only features according to final threshold chosen of 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X_2) #X_train\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "#the first 10 components contain approximately 50% of the variance\n",
    "#we'd need about 50 components to retain 90% of the variance (we would recover most of the essential characteristics of X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('scaling', MinMaxScaler()), ('pca', PCA(0.95))])\n",
    "X_PCA = pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Components = \", pca.n_components_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To Conclude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 54675 numerical features (genes) and only 76 samples. As such, for task 2 we will perform dimensionality reduction. We don't have any information regarding to the scale in which the different genes are measured on but we will assume they are different since the variance of the features ranges from low numbers (0.003) to very high (7.9).\n",
    "\n",
    "\n",
    "- Keeping in mind that a low variance feature hints that that feature lacks information and that variance is range dependent, we will be scalling the features to compare their information on the same scale before removing features with low variance.\n",
    "\n",
    "\n",
    "- We have chosen MinMaxScaler method in which all features are scaled to a given range between 0 and 1 (or -1 to 1 if there are negative values).\n",
    "\n",
    "\n",
    "- Additionally, we have used our own judgement to select the threshold = 0.04 to remove features with low variance - the choice was based on the analysis made backed up by the dataframe low_var_df and plots showing how many columns were to be dropped vs remaining for each of the different thresholds. The goal of dimensionality reduction is to simplify the data without losing too much information and with a threshold equal to 0.04 we still have more columns remaining than removed, from 0.05 onwards we would be removing more columns than those remanining.\n",
    "\n",
    "\n",
    "- One important note would be that to choose an adquate theshold we could also check which threshold would maximize a certain algorithm's performance metric on the test set, however, for this exercise we will not be using any machine learning algorithm.\n",
    "\n",
    "\n",
    "- The other dimensionality reduction technique we have applied was Principal Component Analysis which is a statistical procedure that uses an orthogonal transformation to move the original 54675 coordinates of X into a new set of 67 coordinates called principal components. \n",
    "- PCA is also sensitive to the relative scaling of the original variables of X so we will also use the normalized data X_2.\n",
    "\n",
    "\n",
    "- Having decided to keep 95% of the explained variance of the original dataframe X on a new dataframe called X_PCA was created, we have reduced X dimensionality by finding a new smaller set of variables retaining most of the information on X. By allowing a loss of 5% of the original data set information, we were able to reduce the number of columns from 54675 to 67\n",
    "\n",
    "\n",
    "- For this project we will be using different clustering algorithms to try to detect groups of similar samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering Samples using Partitional Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **`K`-means** to cluster the samples:\n",
    "\n",
    "* Cluster the original data (54.675 features): **X**.\n",
    "    * Use different values of `K`.\n",
    "    * For each value of `K` present the clustering by specifying how many samples MB-CL and Other are in each cluster.     \n",
    "    For instance, `{0: {'MB-CL': 51, 'Other': 0}, 1: {'MB-CL': 0, 'Other': 25}}` is the ideal clustering that we aimed at obtained with K-means when `K=2`, where the first cluster has 51 MB-CL samples and 0 Other samples and the second cluster has 0 MB-CL samples and 25 Other samples.\n",
    "    You can choose how to output this information.  **Tip**: You can explore the usage of contigency matrices.\n",
    "    * What is the best value of `K` ? Justify using the clustering results and the [Silhouette score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).\n",
    "\n",
    "* Cluster the data obtained after removing features with low variance: **X_variance**.\n",
    "    * Study different values of `K` as above.\n",
    "\n",
    "* Cluster the data obtained after applying PCA: **X_PCA**.\n",
    "    * Study different values of `K` as above.\n",
    "\n",
    "* Compare the results obtained in the three datasets above for the best `K`. \n",
    "* Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_per_cluster_kmeans(k, X): \n",
    "    \n",
    "    \"\"\"get_samples_per_cluster_kmeans gets as input the number of clusters k and the dataframe X\n",
    "    and returns a dictionary for each value of K specifying how many samples MB-CL and Other are in each cluster.\"\"\"\n",
    "    \n",
    "    labels = np.array(y['class'])\n",
    "    for k in range(2, k):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "        pred_classes = kmeans.labels_\n",
    "        print('For K =', k)\n",
    "        for i in np.array(np.unique(pred_classes)):\n",
    "            print('Cluster ', i, dict(Counter(labels[np.where(pred_classes == i)]))) #labels -- ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_kmeans(k, X):\n",
    "    \"\"\"get_highest_ss_dbscan gets input a dataframe X as outputs the DBSCAN eps and min_sample parameters \n",
    "    that yield the highest SS alongside that score\"\"\"\n",
    "    # Defining the list of hyperparameters to try\n",
    "    k_list=np.arange(start=2, stop=k, step=1)\n",
    "    # Creating empty data frame to store the silhouette scores for each trials\n",
    "    silhouette_scores_data=pd.DataFrame()\n",
    "\n",
    "    for k_element in k_list:\n",
    "        kmeans = KMeans(n_clusters=k_element, random_state=0).fit(X)\n",
    "        sil_score=silhouette_score(X, kmeans.fit_predict(X))\n",
    "        silhouette_scores_data=silhouette_scores_data.append(pd.DataFrame(data=[[sil_score,k_element]], columns=[\"SS\", \"K\"]))\n",
    "    plt.figure()\n",
    "    plt.plot(silhouette_scores_data.K, silhouette_scores_data.SS)\n",
    "    plt.xlabel(\"Number of clusters K\")\n",
    "    plt.ylabel(\"Silhouette scores\")\n",
    "    plt.xticks(range(2,k+1))\n",
    "    plt.show()  \n",
    "    return silhouette_scores_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparing_ss_kmeans(k,X1, X2, X3):\n",
    "    \n",
    "    \"\"\"comparing_ss_kmeans gets as input the number of clusters k 3 different dataframes named X1, X2 and X3\n",
    "    and returns a plot with the avg SS for each of value of K for each of the 3 dataframes respectively\"\"\"\n",
    "    \n",
    "    slc1, slc2, slc3 = {}, {}, {}\n",
    "    for k in range(2, k):\n",
    "        kmeans1 = KMeans(n_clusters=k, random_state=0).fit(X1)\n",
    "        pred_classes1 = kmeans1.labels_\n",
    "        slc1[k] = silhouette_score(X1,pred_classes1)\n",
    "        \n",
    "        kmeans2 = KMeans(n_clusters=k, random_state=0).fit(X2)\n",
    "        pred_classes2 = kmeans2.labels_\n",
    "        slc2[k] = silhouette_score(X2,pred_classes2)\n",
    "        \n",
    "        kmeans3 = KMeans(n_clusters=k, random_state=0).fit(X3)\n",
    "        pred_classes3 = kmeans3.labels_\n",
    "        slc3[k] = silhouette_score(X3,pred_classes3)\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.plot(list(slc1.keys()), list(slc1.values()), label='X')\n",
    "    plt.xticks(range(2,k+1))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.plot(list(slc2.keys()), list(slc2.values()), label='X_variance')\n",
    "    plt.xticks(range(2,k+1))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.plot(list(slc3.keys()), list(slc3.values()), label='X_PCA')\n",
    "    plt.xticks(range(2,k+1))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"Number of Clusters K\")\n",
    "    plt.ylabel(\"Silhouette Scores\")\n",
    "    plt.title('Comparing SS')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the original data X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_samples_per_cluster_kmeans(6, X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ss_kmeans(6,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For X_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_samples_per_cluster_kmeans(6, X_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ss_kmeans(6,X_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - For X_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_samples_per_cluster_kmeans(6, X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ss_kmeans(6,X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing_ss_kmeans(6,X, X_variance, X_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Partitional clustering divides data objects into nonoverlapping groups. As such, an object can be a member of more than one cluster, and every cluster must have at least one object.\n",
    "\n",
    "- In specific, k-means is a Hard partition algorithm which means it assigns a unique cluster value to each element in the feature space. The K-means algorithm divides a set of N samples X into K disjoint clusters C, each described by the mean of the samples in the cluster.\n",
    "\n",
    "\n",
    "- Silhouette score is used to evaluate the quality of clusters (how well the samples are clustered with other samples that are similar to each other) - the best value for the SS is 1 and that values near 0 indicate overlapping clusters\n",
    "\n",
    "\n",
    "- For all three datasets, we observe that the SS scores are low which could hint that the samples are not well clustered with other samples that are similar to each other.\n",
    "\n",
    "\n",
    "- The **best K** (the one that yieled the highest Silhouette Score) was 2 for both **X and X_PCA**. While the highest SS for X_variance was when K=4 and K=5.\n",
    "\n",
    "\n",
    "- **K-means seemed to have performed better on the original dataframe X** for all the represented number of Ks in the plot, having reaching the highest SS of 0.1 when K=2 as previously mentioned. \n",
    "\n",
    "\n",
    "- The K-means algorithm aims to choose centroids minimizing a criterion known as the inertia. Inertia makes the assumption that clusters are convex and isotropic. Such low SS for all three dataframes may indicate that in fact we may be in the presence of elongated and overlapping clusters with irregular shapes (and not spherical clusters).\n",
    "\n",
    "\n",
    "- As an example, with K=2 for the original dataset X we get that Cluster  0 {'MB-CL': 32, 'Other': 9} and Cluster  1 {'MB-CL': 19, 'Other': 16}, meaning that the first cluster has 32 MB-CL samples and 9 Other samples and the second cluster has 19 MB-CL samples and 16 Other samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Samples using Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a **Hierarchical Clustering Algorithm (HCA)** to cluster the samples: \n",
    "\n",
    "* Cluster the data in **X_variance**.\n",
    "    * Use **different linkage metrics**.\n",
    "    * Use different values of `K`.\n",
    "    * For each linkage metric and value of `K` present the clustering by specifying how many MB-CL and Other samples are in each cluster as you did before. \n",
    "    * What is the best linkage metric and the best value of `K`? Justify using the clustering results and the [Silhouette score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).\n",
    "\n",
    "* Cluster the data in **X_PCA**.\n",
    "    * Study different linkage metrics and different values of `K` as above.\n",
    "\n",
    "* Compare the results obtained in the two datasets above for the best linkage metric and the best `K`. \n",
    "* Discuss the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_per_cluster_hca(k, X):\n",
    "    \n",
    "    \"\"\"get_samples_per_cluster_hca gets as input the number of clusters k and the dataframe X\n",
    "    and returns a dictionary for each linkage method and for each value of K \n",
    "    specifying how many samples MB-CL and Other are in each cluster.\"\"\"\n",
    "    \n",
    "    labels = np.array(y['class'])\n",
    "    linkage = ['ward', 'complete', 'average', 'single']\n",
    "    for k in range(2, k):\n",
    "        print('For K =', k)\n",
    "        for l in linkage:\n",
    "            print('Linkage method =',l)\n",
    "            clustering = AgglomerativeClustering(linkage=l, n_clusters=k).fit(X)\n",
    "            pred_classes = clustering.labels_\n",
    "            for i in np.array(np.unique(pred_classes)):\n",
    "                print('Cluster ', i, dict(Counter(labels[np.where(pred_classes == i)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparing_ss_hca_methods(k,X):\n",
    "    \n",
    "    \"\"\"comparing_ss_hca_methods gets as input the number of clusters k and the dataframe X\n",
    "    and returns a plot with the avg SS for each of value of K for each of the 3 linkage methods respectively\"\"\"\n",
    "        \n",
    "    slc1, slc2, slc3, slc4 = {}, {}, {}, {}\n",
    "    for k in range(2, k):\n",
    "        hca1 = AgglomerativeClustering(n_clusters=k, linkage='ward').fit(X)\n",
    "        pred_classes1 = hca1.labels_\n",
    "        slc1[k] = silhouette_score(X,pred_classes1)\n",
    "\n",
    "        hca2 = AgglomerativeClustering(n_clusters=k, linkage='complete').fit(X)\n",
    "        pred_classes2 = hca2.labels_\n",
    "        slc2[k] = silhouette_score(X,pred_classes2)\n",
    "\n",
    "        hca3 = AgglomerativeClustering(n_clusters=k, linkage='average').fit(X)\n",
    "        pred_classes3 = hca3.labels_\n",
    "        slc3[k] = silhouette_score(X,pred_classes3)\n",
    "             \n",
    "    plt.figure()\n",
    "    plt.plot(list(slc1.keys()), list(slc1.values()), label='Ward method')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xticks(range(2,k+1))\n",
    "    plt.plot(list(slc2.keys()), list(slc2.values()), label='Complete method')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xticks(range(2,k+1))\n",
    "    plt.plot(list(slc3.keys()), list(slc3.values()), label='Average method')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xticks(range(2,k+1))\n",
    "\n",
    "    plt.xlabel(\"Number of Clusters K\")\n",
    "    plt.ylabel(\"Silhouette Scores\")\n",
    "    plt.title('Comparing SS')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - For X_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing_ss_hca_methods(6,X_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_samples_per_cluster_hca(6, X_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - For X_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing_ss_hca_methods(6,X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_samples_per_cluster_hca(6, X_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Agglomerative Clustering performs a hierarchical clustering using a bottom up approach where each observation starts in its own cluster and clusters are successively merged together.\n",
    "\n",
    "- The linkage criterion determines the metric used for the merge strategy, from the \n",
    "[scikit learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html):\n",
    "\n",
    "    *- ‘ward’ minimizes the variance of the clusters being merged.*\n",
    "\n",
    "    *- ‘average’ uses the average of the distances of each observation of the two sets.*\n",
    "\n",
    "    *- ‘complete’ or ‘maximum’ linkage uses the maximum distances between all observations of the two sets.*\n",
    "\n",
    "    *- ‘single’ uses the minimum of the distances between all observations of the two sets.*\n",
    "\n",
    "\n",
    "- For **X_variance** the best K (the one that yieled the highest SS) was 2 for all the linkage methods (distance used between sets of observation). **For all K values represented the best linkage method was the Average method** where **SS=0.16** (which uses the average of the distances of each observation) followed by the complete method (which uses the maximum distances between all observations of the two sets) when K=2 which was close to 0.16.\n",
    "\n",
    "\n",
    "- As an example, with K=2 and the average method we get that Cluster 0 {'MB-CL': 50, 'Other': 25} and Cluster 1 {'MB-CL': 1}, meaning that the first cluster has 50 MB-CL samples and 25 Other samples and the second cluster has 1 MB-CL samples and 0 Other samples. Additionally, with the same K=2 and the complete method we get that Cluster 0 {'MB-CL': 49, 'Other': 25} and Cluster 1 {'MB-CL': 2}, meaning that the first cluster has 49 MB-CL samples and 25 Other samples and the second cluster has 1 MB-CL samples and 0 Other samples.\n",
    "\n",
    "\n",
    "- **Comparing the SS, we see that the performance was better on X_PCA**.\n",
    "\n",
    "\n",
    "- **For X_PCA the best K (the one that yieled the highest SS) was 2 for the average and complete methods**. The avg SS didn't seem to change regardless of the K number of clusters for the ward method.\n",
    "\n",
    "\n",
    "- **For all K values represented the best linkage method was again the Average method** where SS=0.19 followed by the complete method when K=2 which was close to 0.17.\n",
    "\n",
    "\n",
    "- We can provide a similar interpretation of the clusters formed according to the get_samples_per_cluster_hca function. For K=2 and the average method we get that Cluster 0 {'MB-CL': 49, 'Other': 25} and Cluster 1 {'MB-CL': 2}, meaning that the first cluster has 49 MB-CL samples and 25 Other samples and the second cluster has 1 MB-CL samples and 0 Other samples. This was the same cluster formation we got for X_variance when K=2 with the complete method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Clustering Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you should compare the best results obtained using `K`-means and HCA \n",
    "1. **Without using ground truth**\n",
    "2. **Using ground truth (`Medulloblastoma Type`)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Without Using Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose one adequate measure** from those available by Sciki-learn (https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation) to evaluate the different clusterings. \n",
    "\n",
    "Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ss_kmeans(6,X_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing_ss_hca_methods(6,X_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ss_kmeans(6,X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing_ss_hca_methods(6,X_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Considering that we have been analysing the Silhouette Score for both algorithms (K-means and HCA) so far, we have chosen to continue the comparasion between algorithms for this task using the same SS metric.\n",
    "- As such, we observe that **for both dataframes X_variance and X_PCA, the Hierarchical Clustering Algorithm (HCA) yieled the highest avg SS for all K clusters represented when compared to K-means.**\n",
    "- In specific, for **X_variance** the highest SS was 0.16 using the HCA with the average method and k=2 while the highest SS was 0.05 when k=4 using the k-means algorithm.\n",
    "- Similarly, for **X_PCA** the highest SS was 0.19 using the HCA with the average method and k=2 while the highest SS was 0.07 when k=4 using the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Using Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose one adequate measure** from those available by Sciki-learn (https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation) to evaluate the different clusterings. \n",
    "\n",
    "Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparing_kmeans_ground_truth(k,X):\n",
    "    \n",
    "    \"\"\"comparing_kmeans_ground_truth gets as input the number of clusters k and the dataframe X\n",
    "    and chooses the k that yields the highest SS using the k-means algorithm\n",
    "    to then print out metrics to evaluate the predicted labels vs the true labels.\"\"\"\n",
    "    \n",
    "    labels_true=y['class']\n",
    "    slc = {}\n",
    "    for k in range(2, k):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "        pred_classes = kmeans.labels_\n",
    "        slc[k] = silhouette_score(X,pred_classes)\n",
    "    \n",
    "    highest_ss_k = max(slc.items(), key=operator.itemgetter(1))[0]\n",
    "    kmeans = KMeans(n_clusters=highest_ss_k, random_state=0).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    print('For K =', highest_ss_k)\n",
    "    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "    #print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "    #print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "    #print(\"Adjusted Rand Index: %0.3f\"\n",
    "    #      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "    #print(\"Adjusted Mutual Information: %0.3f\"\n",
    "    #  % metrics.adjusted_mutual_info_score(labels_true, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing_kmeans_ground_truth(11,X_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing_kmeans_ground_truth(11,X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparing_hca_ground_truth(k,X):\n",
    "    \n",
    "    \"\"\"comparing_hca_ground_truth gets as input the number of clusters k and the dataframe X\n",
    "    and chooses the k that yields the highest SS using the HCA\n",
    "    to then print out metrics to evaluate the predicted labels vs the true labels.\"\"\"\n",
    "    \n",
    "    labels_true=y['class']\n",
    "    slc = {}\n",
    "    for k in range(2, k):\n",
    "        hca = AgglomerativeClustering(n_clusters=k).fit(X)\n",
    "        pred_classes = hca.labels_\n",
    "        slc[k] = silhouette_score(X,pred_classes)\n",
    "        \n",
    "    highest_ss_k = max(slc.items(), key=operator.itemgetter(1))[0]\n",
    "    hca1 = AgglomerativeClustering(n_clusters=highest_ss_k).fit(X)\n",
    "    labels = hca1.labels_\n",
    "    \n",
    "    print('For K =', highest_ss_k)\n",
    "    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "    #print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "    #print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "    #print(\"Adjusted Rand Index: %0.3f\"\n",
    "    #      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "    #print(\"Adjusted Mutual Information: %0.3f\"\n",
    "    #  % metrics.adjusted_mutual_info_score(labels_true, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing_hca_ground_truth(11,X_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing_hca_ground_truth(11,X_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering helps us to analyze and get insight of the data, but the quality of the partition depends on the application of those insights/problem.\n",
    "\n",
    "- Given the context of the task, we have chosen that the Homogeneity would be an adequate metric to evaluate the different clusters. \n",
    "\n",
    "- Homogeneous clustering means that each cluster contains only members of a single class. This metric ranges between 0 and 1, with low values indicating a low homogeneity.\n",
    "\n",
    "- Ultimately, using different clustering algorithms we want to try to detect groups of similar samples and characterize them while answering questions such as \"do the MB-CL gene samples actually behave differently than those classified as 'Others'?\" and \"do we have groups of genes behaving similarly even though the sample type is different?\"\n",
    "\n",
    "- We want to study if each cluster contains samples of a single class (whether that would be 'MB-CL' or 'Others').\n",
    "\n",
    "- For X_variance, homogeneity was higher (0.153) with the K-means algorithm for K=2 when compared to the value of 0.052 which is a result of the HCA when K=2.\n",
    "\n",
    "- For X_PCA, homogeneity was higher (0.229) with the HCA algorithm for K=10 when compared to the value of 0.018 which is a result of the k-means when K=2.\n",
    "\n",
    "- Overall, the homogeneity was higher for X_PCA with the HCA algorithm for K=10 but we have to take this result carefully since 10 groups seem to be a relatively high number since so far we had know that the samples were initially divided into MB-CL and Others (with 3 subgroups inside others) - this result could be leading to a further split and \"more detailed\" MB sample clusters but we would want to investigate if this would be benefitial for the problem context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering Samples using Density-based Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use DBSCAN (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) or OPTICS (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html) to cluster the samples.\n",
    "\n",
    "Compare the results with those of K-means and HCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_func(X):\n",
    "    \n",
    "    \"\"\"dbscan_func takes as input a given dataframe X\n",
    "    and returns the number of clusters and noise points for each EPS respectively\"\"\"\n",
    "    \n",
    "    # Defining the list of hyperparameters to try\n",
    "    eps_list=np.arange(start=50, stop=150, step=10)\n",
    "    min_sample_list=np.arange(start=5, stop=30, step=5)\n",
    "    \n",
    "    for eps_trial in eps_list:\n",
    "        print('For 𝜖 =', eps_trial)\n",
    "        for min_sample_trial in min_sample_list:\n",
    "            db = DBSCAN(eps=eps_trial, min_samples=min_sample_trial).fit(X)\n",
    "            core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "            core_samples_mask[db.core_sample_indices_] = True\n",
    "            labels = db.labels_\n",
    "            n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) \n",
    "            n_noise_ = list(labels).count(-1) #clusters labeled w/-1  don't belong to a cluster\n",
    "            print('with min_sample = ', min_sample_trial, '| Clusters =', n_clusters_, 'Noise points =', n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_ss_dbscan(X):\n",
    "    \"\"\"get_highest_ss_dbscan gets input a dataframe X as outputs the DBSCAN eps and min_sample parameters \n",
    "    that yield the highest SS alongside that score\"\"\"\n",
    "    # Defining the list of hyperparameters to try\n",
    "    eps_list=np.arange(start=50, stop=150, step=10)\n",
    "    min_sample_list=np.arange(start=5, stop=30, step=5)\n",
    "\n",
    "    # Creating empty data frame to store the silhouette scores for each trials\n",
    "    silhouette_scores_data=pd.DataFrame()\n",
    "\n",
    "    for eps_trial in eps_list:\n",
    "        for min_sample_trial in min_sample_list:\n",
    "            db = DBSCAN(eps=eps_trial, min_samples=min_sample_trial)\n",
    "            if(len(np.unique(db.fit_predict(X)))>1):\n",
    "                sil_score=silhouette_score(X, db.fit_predict(X))\n",
    "            else:\n",
    "                continue\n",
    "            trial_parameters=\"eps:\" + str(eps_trial.round(1)) +\" min_sample :\" + str(min_sample_trial)\n",
    "\n",
    "            silhouette_scores_data=silhouette_scores_data.append(pd.DataFrame(data=[[sil_score,trial_parameters]], columns=[\"Highest SS for DBSCAN\", \"Parameters\"]))\n",
    "    print(silhouette_scores_data.sort_values(by=\"Highest SS for DBSCAN\", ascending=False).head(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_func(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_func(X_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_func(X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_highest_ss_dbscan(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_highest_ss_dbscan(X_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_highest_ss_dbscan(X_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have previously seen with k-means and HCA that we need to specify the number of clusters K in order to use the algorithm, this isn't the case with DBSCAN which can be seen as an advantage of this algorithm. Additionally, clusters found by DBSCAN can be of any shape, as opposed to k-means which assumes that clusters are convex shaped as we've also covered in task 3.\n",
    "\n",
    "- To compute DBSCAN we start from one point and the algorithm attempts to build a cluster by grouping its 𝜖−neighborhoods to check if it is a core point or not. If no such point is available, it is labeled as noise. If some are available (core points), for these points their directly reachable points are added, and we continue until we cannot reach anymore points. Then we move on to the next point that we haven't labeled yet and DBSCAN performs the same steps, until all points have been visited. By the end, we then know the cluster numbers and the noisy points.\n",
    "\n",
    "- DBSCAN is robust to outliers and is also capable of detecting noisy points. Given this, we've tried changing the parameters of DBSCAN, in particular we have tried to find a meaningful distance threshold 𝜖 taking into consideration that DBSCAN is in fact  sensitive to scale. \n",
    "\n",
    "- For smaller values of 𝜖, all 76 points were being considered as noise, they weren't part of any cluster. As such, using the function *dbscan_func()* we have shown how the different   𝜖 values (starting from 50 until 150) influence the algorithm and its insights for the three different dataframes. We've also have also tried to change the minimum number of samples (depending on the density of the clusters).\n",
    "\n",
    "- For the **original dataframe X**, until 𝜖 =  130 all points were being considered as noise. However, when For 𝜖 = 140, DBSCAN already created 1 clusters with 5 observations and left 71 observations as noise points. \n",
    "\n",
    "- For the reduced dataframe **X_variance**, until 𝜖 = 50, all points were being considered as noise. When 𝜖 = 50, DBSCAN considered two cluster formations of 66 samples and left 10 observations as noise points. With values higher than 60 for the 𝜖 parameter, DBSCAN based clustering was aggregating all samples into one big cluster, leaving none as noise - which for this particular problem was not a useful solution to have all 76 observations clustered into one big cluster.\n",
    "\n",
    "- It's important to remember that both X and X_variance are high-dimensionality dataframes and that when using the DBSCAN algorithm we may be faced with the so-called 'curse of high dimensionality' making it difficult to find an appropriate value for eps. As mentioned, we have chosen the range of eps values via trial and error until we found an interval in which the DBSCAN algorithm was, in fact, separating observations and not considering all 76 points as noise points.\n",
    "\n",
    "- Lastly, for the **X_PCA** dataframe we get one interesting insight: when 𝜖 = 50 and min_samples=5, we get that DBSCAN clusters 22 samples of the the data into two different clusters, having left 54 points as 'noise'. Increasing the 𝜖 value to 60, we observe that DBSCAN now yields 1 cluster with 58 observations and 18 noise points.\n",
    "\n",
    "- Up until this task we had used the Silhouette score to evaluate the clustering algorithms. On key point is that the SS assumes that all points are assigned to a cluster (so there are not 'noise points') so we cannot use the SS to judge the quality of results yieled by the DBSCAN algorithm even though we have computed it through the *get_highest_ss_dbscan()* function. We have previously touched on the fact that SS assumes convex clusters, but DBSCAN does not generate convex clusters.\n",
    "\n",
    "- We can conclude that DBSCAN algorithm's performance was worse than K-means and HCA in the sense that despite the existence of 2 labels in our original dataset (MB-CL and Others), for most 𝜖 DBSCAN uncovered only 1 cluster for all  and when it did uncover 2 clusters a high number of samples are left as 'noise points' (54 noise points for X_PCA when 𝜖 = 50 and k=2; 76 noise points for X_PCA when 𝜖 = 60 and k=2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Choose a Different Clustering Algorithm to Group the Samples\n",
    "\n",
    "Choose **a clustering algorithm** besides `K`-means, HCA and DBSCAN/OPTICS to cluster the samples. \n",
    "\n",
    "**Groups of 3 People** must choose two different algorithms.\n",
    "\n",
    "Justify your choice and compare the results with those of `K`-means, HCA and DBSCAN/OPTICS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_per_cluster_mini_kmeans(k, X):\n",
    "    \n",
    "    \"\"\"get_samples_per_cluster_mini_kmeans gets as input the number of clusters k and the dataframe X\n",
    "    and returns a dictionary for each value of K specifying how many samples MB-CL and Other are in each cluster\n",
    "    using the mini batch k-means algorithm\"\"\"\n",
    "    \n",
    "    labels = np.array(y['class'])\n",
    "    for k in range(2, k):\n",
    "        mini_kmeans = MiniBatchKMeans(n_clusters=k, random_state=0).fit(X)\n",
    "        pred_classes = mini_kmeans.labels_\n",
    "\n",
    "        print('For K =', k)\n",
    "        for i in np.array(np.unique(pred_classes)):\n",
    "            print('Cluster ', i, dict(Counter(labels[np.where(pred_classes == i)]))) #labels -- ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_mini_kmeans(X):\n",
    "    \"\"\"get_highest_ss_dbscan gets input a dataframe X as outputs the DBSCAN eps and min_sample parameters \n",
    "    that yield the highest SS alongside that score\"\"\"\n",
    "    # Defining the list of hyperparameters to try\n",
    "    k_list=np.arange(start=2, stop=6, step=1)\n",
    "    # Creating empty data frame to store the silhouette scores for each trials\n",
    "    silhouette_scores_data=pd.DataFrame()\n",
    "\n",
    "    for k in k_list:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=k, random_state=0).fit(X)\n",
    "        sil_score=silhouette_score(X, kmeans.fit_predict(X))\n",
    "        silhouette_scores_data=silhouette_scores_data.append(pd.DataFrame(data=[[sil_score,k]], columns=[\"SS\", \"K\"]))\n",
    "    plt.figure()\n",
    "    plt.plot(silhouette_scores_data.K, silhouette_scores_data.SS)\n",
    "    plt.xlabel(\"Number of clusters K\")\n",
    "    plt.ylabel(\"Silhouette scores\")\n",
    "    plt.xticks(range(2,6))\n",
    "    plt.show()  \n",
    "    return silhouette_scores_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_per_cluster_spec(k, X):\n",
    "        \n",
    "    \"\"\"get_samples_per_cluster_spec gets as input the number of clusters k and the dataframe X\n",
    "    and returns a dictionary for each value of K specifying how many samples MB-CL and Other are in each cluster\n",
    "    using the spectral clustering algorithm\"\"\"\n",
    "    \n",
    "    labels = np.array(y['class'])\n",
    "    for k in range(2, k):\n",
    "        mini_kmeans = SpectralClustering(n_clusters=k, random_state=0).fit(X)\n",
    "        pred_classes = mini_kmeans.labels_\n",
    "\n",
    "        print('For K =', k)\n",
    "        for i in np.array(np.unique(pred_classes)):\n",
    "            print('Cluster ', i, dict(Counter(labels[np.where(pred_classes == i)]))) #labels -- ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_spec(k,X):\n",
    "    slc = {}\n",
    "    for k in range(2, k):\n",
    "        kmeans = SpectralClustering(n_clusters=k, random_state=0).fit(X)\n",
    "        pred_classes = kmeans.labels_\n",
    "        slc[k] = silhouette_score(X,pred_classes)\n",
    "        print(\"For K =\", k, \"The avg SS is :\", np.round(silhouette_score(X, pred_classes),2))\n",
    "    plt.figure()\n",
    "    plt.plot(list(slc.keys()), list(slc.values()))\n",
    "    plt.xlabel(\"Number of clusters K\")\n",
    "    plt.ylabel(\"Silhouette scores\")\n",
    "    plt.xticks(range(2,k+1))\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_spectural_clust(X):\n",
    "    \"\"\"get_highest_ss_dbscan gets input a dataframe X as outputs the DBSCAN eps and min_sample parameters \n",
    "    that yield the highest SS alongside that score\"\"\"\n",
    "    # Defining the list of hyperparameters to try\n",
    "    k_list=np.arange(start=2, stop=6, step=1)\n",
    "    # Creating empty data frame to store the silhouette scores for each trials\n",
    "    silhouette_scores_data=pd.DataFrame()\n",
    "\n",
    "    for k in k_list:\n",
    "        kmeans = SpectralClustering(n_clusters=k, random_state=0).fit(X)\n",
    "        sil_score=silhouette_score(X, kmeans.fit_predict(X))\n",
    "        silhouette_scores_data=silhouette_scores_data.append(pd.DataFrame(data=[[sil_score,k]], columns=[\"SS\", \"K\"]))\n",
    "    plt.figure()\n",
    "    plt.plot(silhouette_scores_data.K, silhouette_scores_data.SS)\n",
    "    plt.xlabel(\"Number of clusters K\")\n",
    "    plt.ylabel(\"Silhouette scores\")\n",
    "    plt.xticks(range(2,6))\n",
    "    plt.show()  \n",
    "    return silhouette_scores_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mini batch K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ss_mini_kmeans(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_samples_per_cluster_mini_kmeans(6,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For X_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ss_mini_kmeans(X_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_samples_per_cluster_mini_kmeans(6,X_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For X_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ss_mini_kmeans(X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_samples_per_cluster_mini_kmeans(6,X_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spectural Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "get_samples_per_cluster_spec(6, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ss_spectural_clust(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For X_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "get_samples_per_cluster_spec(6, X_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ss_spectural_clust(X_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For X_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "get_samples_per_cluster_spec(6, X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ss_spectural_clust(X_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The MiniBatchKMeans is a variant of the KMeans algorithm using mini-batches, random subset of the dataset, to compute the centroids.\n",
    "- For the original dataframe **X**, the highest SS was 0.1 for K=2 for both K-means and Mini batch K-means. The Spectural clustering algorithm performed bad as for all the number of clusters K analysed the SS was always negative.\n",
    "- For **X_variance**, the highest SS was 0.16 for K=2 with the average method of the HCA. With SS = 0.05 for K=2 with the mini batch K-means algorithm which performed better than the k-means algorithm where the highest SS was 0.047 for K=4 and K=5. The Spectural clustering algorithm had a relatively bad performance as for all the number of clusters K analysed the SS was always negative except for K=2 when SS = 0.001.\n",
    "- For **X_PCA**, SS was the highest when using the HCA with the average method (SS=0.19) with K=2. Again, with K=2 the mini batch k-means performed better (SS=0.08) compared to the original k-means (SS=0.06). Similar to what we've described to far, the Spectural clustering algorithm had the worst performance being close to zero when K=2 and K=3.\n",
    "- For both X_variance and X_PCA, we had already established that despite the SS that we got, the DBSCAN's performance was not good (putting all data points in one cluster for most of the parameter combinations or classifying all data points as noise). As such, we consider DBSCAN to have the worst performance out of all the algorithms analysed so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw some conclusions about this project work. Can you highlight some insights about meduloblastoma types? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For X_variance, the highest SS was 0.16 for K=2 with the average method of the HCA.\n",
    "#For X_PCA, SS was the highest when using the HCA with the average method (SS=0.19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write text in cells like this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Silhouette Coefficient is generally higher for convex clusters.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied the discussed clustering techniques, namely: K-means, agglomerative clustering with average linkage, agglomerative clustering with complete linkage, agglomerative clustering with Ward linkage and spectral clustering. Connectivity is applied in the algorithms where it is applicable. Regarding the simple cases of separated clusters of data, all clustering algorithms perform well, as expected. Regarding uniform distribution of data, K-means, Ward agglomerative clustering and spectral clustering tend to obtain even and compact clusters, while complete linkage and average linkage agglomerative clusterings try to agglomerate as much as possible close points following the rule: “rich get richer”. This results in a second cluster of a small set of data. Regarding the embedded structures as in the case of the circles (the third row), K-means and Ward linkage agglomerative clustering try to obtain compact clusters and thus cannot separate the circles. Similar effect is observed with the moons example (the fourth row). Another fact worthful to emphasize from the illustrating example is the importance of knowing the number of clusters looked for, in the case of K-means, ward-linkage agglomerative clustering and spectral clustering, since the later also employs the K-means method. If k is not known, the average-linkage or complete-linkage agglomerative clustering is recommendable with the risk that it will not assure balanced clusters as final result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

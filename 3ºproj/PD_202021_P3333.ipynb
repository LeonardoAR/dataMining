{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining / Prospeção de Dados\n",
    "\n",
    "## Diogo F. Soares and Sara C. Madeira, 2020/21\n",
    "\n",
    "# Project 3 - Classification/Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics\n",
    "\n",
    "**_Read Carefully_**\n",
    "\n",
    "**Students should work in teams of 2 or 3 people**. \n",
    "\n",
    "Individual projects might be allowed (with valid justification), but will not have better grades for this reason. \n",
    "\n",
    "* Tasks **1 to 4** should be done by **all** groups **BUT**\n",
    "    * In Tasks **2 and 3** the number of classifiers/regressors to use is **`X` = 3 and 4**  for groups of 2 and 3 students, respectively.\n",
    "    * In Task **4**, groups of 2 and 3 students should try 2 and 3 ensemble approaches, respectively.\n",
    "\n",
    "The quality of the project will dictate its grade, not the number of people working.\n",
    "\n",
    "**The project's solution should be uploaded in Moodle before the end of  `May, 23th (23:59)`.** \n",
    "\n",
    "Students should **upload a `.zip` file** containing all the files necessary for project evaluation. \n",
    "Groups should be registered in [Moodle](https://moodle.ciencias.ulisboa.pt/mod/groupselect/view.php?id=139096) and the zip file should be identified as `PDnn.zip` where `nn` is the number of your group.\n",
    "\n",
    "**It is mandatory to produce a Jupyter notebook containing code and text/images/tables/etc describing the solution and the results. Projects not delivered in this format will not be graded. You can use `PD_202021_P3.ipynb`as template. In your `.zip` folder you should also include an HTML version of your notebook with all the outputs** (File > Download as > HTML).\n",
    "\n",
    "**Decisions should be justified and results should be critically discussed.** \n",
    "\n",
    "_Project solutions containing only code and outputs without discussions will achieve a maximum grade 10 out of 20._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Tools\n",
    "\n",
    "In this project you should use [Python 3](https://www.python.org), [Jupyter Notebook](http://jupyter.org) and **[Scikit-learn](http://scikit-learn.org/stable/). You are also allowed to use [Orange3](https://orange.biolab.si).**\n",
    "\n",
    "The dataset to be analysed is **`RestaurantsRevenue.csv`**, a modified version of the test dataset used in Kaggle's competition [\"Restaurant Revenue Prediction\"](https://www.kaggle.com/c/restaurant-revenue-prediction/overview). \n",
    "\n",
    "**This project challenges you twice** by asking you to tackle a\n",
    "1. **Regression Task**: predict the revenue, and a\n",
    "2. **Classification Task**: predict a revenue category.\n",
    "\n",
    "The available variables are:\n",
    "\n",
    "* **Id :** Restaurant id. \n",
    "* **Open Date :** opening date for a restaurant\n",
    "* **City :** City that the restaurant is in. Note that there are unicode in the names. \n",
    "* **City Group:** Type of the city. Big cities, or Other. \n",
    "* **Type:** Type of the restaurant. FC: Food Court, IL: Inline, DT: Drive Thru, MB: Mobile\n",
    "* **P1, P2 - P37:** There are three categories of these obfuscated data. Demographic data are gathered from third party providers with GIS systems. These include population in any given area, age and gender distribution, development scales. Real estate data mainly relate to the m2 of the location, front facade of the location, car park availability. Commercial data mainly include the existence of points of interest including schools, banks, other QSR operators.\n",
    "\n",
    "\n",
    "The targets are:\n",
    "1. **`Revenue`:** The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. Please note that the values are transformed so they don't mean real dollar values. \n",
    "2. **`RevenueCategory`** - the revenue category, where price can be below 12000 (\"<12K\"), between 12000 and 20000 (\"12K-20K\"), or above 20000 (\">20K\"). This is the target variable that you're trying to predict in the classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Identification\n",
    "\n",
    "**GROUP NNN**\n",
    "\n",
    "Students:\n",
    "\n",
    "* Student 1 - n_student1\n",
    "* Student 2 - n_student2\n",
    "* Student 3 - n_student3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you might consider useful to think about:\n",
    "1. Missing Value Imputation\n",
    "2. Feature Selection\n",
    "3. Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before going through the 3 tasks above, we will explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('RestaurantsReveneu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Open Date</th>\n",
       "      <th>City</th>\n",
       "      <th>City Group</th>\n",
       "      <th>Type</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>...</th>\n",
       "      <th>P29</th>\n",
       "      <th>P30</th>\n",
       "      <th>P31</th>\n",
       "      <th>P32</th>\n",
       "      <th>P33</th>\n",
       "      <th>P34</th>\n",
       "      <th>P35</th>\n",
       "      <th>P36</th>\n",
       "      <th>P37</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>01/22/2011</td>\n",
       "      <td>Niğde</td>\n",
       "      <td>Other</td>\n",
       "      <td>FC</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10033.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>03/18/2011</td>\n",
       "      <td>Konya</td>\n",
       "      <td>Other</td>\n",
       "      <td>IL</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9355.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10/30/2013</td>\n",
       "      <td>Ankara</td>\n",
       "      <td>Big Cities</td>\n",
       "      <td>FC</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>05/06/2013</td>\n",
       "      <td>Kocaeli</td>\n",
       "      <td>Other</td>\n",
       "      <td>IL</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>07/31/2013</td>\n",
       "      <td>Afyonkarahisar</td>\n",
       "      <td>Other</td>\n",
       "      <td>FC</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9354.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Open Date            City  City Group Type  P1   P2   P3   P4  P5  \\\n",
       "0   0  01/22/2011           Niğde       Other   FC   1  4.0  4.0  4.0   1   \n",
       "1   1  03/18/2011           Konya       Other   IL   3  4.0  4.0  4.0   2   \n",
       "2   2  10/30/2013          Ankara  Big Cities   FC   3  4.0  4.0  4.0   2   \n",
       "3   3  05/06/2013         Kocaeli       Other   IL   2  4.0  4.0  4.0   2   \n",
       "4   4  07/31/2013  Afyonkarahisar       Other   FC   2  4.0  4.0  4.0   1   \n",
       "\n",
       "   ...  P29  P30  P31  P32  P33  P34  P35  P36  P37  revenue  \n",
       "0  ...  3.0    0    0    0    0    0    0    0    0  10033.0  \n",
       "1  ...  3.0    0    0    0    0    0    0    0    0   9355.0  \n",
       "2  ...  3.0    0    0    0    0    0    0    0    0  11353.0  \n",
       "3  ...  3.0    0    4    0    0    0    0    0    0  10828.0  \n",
       "4  ...  3.0    0    0    0    0    0    0    0    0   9354.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 43)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 43 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   Id          100000 non-null  int64  \n",
      " 1   Open Date   100000 non-null  object \n",
      " 2   City        100000 non-null  object \n",
      " 3   City Group  100000 non-null  object \n",
      " 4   Type        100000 non-null  object \n",
      " 5   P1          100000 non-null  int64  \n",
      " 6   P2          100000 non-null  float64\n",
      " 7   P3          100000 non-null  float64\n",
      " 8   P4          100000 non-null  float64\n",
      " 9   P5          100000 non-null  int64  \n",
      " 10  P6          100000 non-null  int64  \n",
      " 11  P7          100000 non-null  int64  \n",
      " 12  P8          100000 non-null  int64  \n",
      " 13  P9          100000 non-null  int64  \n",
      " 14  P10         100000 non-null  int64  \n",
      " 15  P11         100000 non-null  int64  \n",
      " 16  P12         100000 non-null  int64  \n",
      " 17  P13         100000 non-null  float64\n",
      " 18  P14         100000 non-null  int64  \n",
      " 19  P15         100000 non-null  int64  \n",
      " 20  P16         100000 non-null  int64  \n",
      " 21  P17         100000 non-null  int64  \n",
      " 22  P18         100000 non-null  int64  \n",
      " 23  P19         100000 non-null  int64  \n",
      " 24  P20         100000 non-null  int64  \n",
      " 25  P21         100000 non-null  int64  \n",
      " 26  P22         100000 non-null  int64  \n",
      " 27  P23         100000 non-null  int64  \n",
      " 28  P24         100000 non-null  int64  \n",
      " 29  P25         100000 non-null  int64  \n",
      " 30  P26         100000 non-null  float64\n",
      " 31  P27         100000 non-null  float64\n",
      " 32  P28         100000 non-null  float64\n",
      " 33  P29         100000 non-null  float64\n",
      " 34  P30         100000 non-null  int64  \n",
      " 35  P31         100000 non-null  int64  \n",
      " 36  P32         100000 non-null  int64  \n",
      " 37  P33         100000 non-null  int64  \n",
      " 38  P34         100000 non-null  int64  \n",
      " 39  P35         100000 non-null  int64  \n",
      " 40  P36         100000 non-null  int64  \n",
      " 41  P37         100000 non-null  int64  \n",
      " 42  revenue     100000 non-null  float64\n",
      "dtypes: float64(9), int64(30), object(4)\n",
      "memory usage: 32.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>...</th>\n",
       "      <th>P29</th>\n",
       "      <th>P30</th>\n",
       "      <th>P31</th>\n",
       "      <th>P32</th>\n",
       "      <th>P33</th>\n",
       "      <th>P34</th>\n",
       "      <th>P35</th>\n",
       "      <th>P36</th>\n",
       "      <th>P37</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>49999.500000</td>\n",
       "      <td>4.088030</td>\n",
       "      <td>4.428085</td>\n",
       "      <td>4.215325</td>\n",
       "      <td>4.396025</td>\n",
       "      <td>1.989590</td>\n",
       "      <td>2.881900</td>\n",
       "      <td>5.30051</td>\n",
       "      <td>4.93100</td>\n",
       "      <td>5.251380</td>\n",
       "      <td>...</td>\n",
       "      <td>3.084000</td>\n",
       "      <td>2.083300</td>\n",
       "      <td>1.193330</td>\n",
       "      <td>1.942640</td>\n",
       "      <td>0.987430</td>\n",
       "      <td>2.108670</td>\n",
       "      <td>1.832830</td>\n",
       "      <td>1.968890</td>\n",
       "      <td>0.973500</td>\n",
       "      <td>14698.061620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28867.657797</td>\n",
       "      <td>2.812963</td>\n",
       "      <td>1.428865</td>\n",
       "      <td>0.842161</td>\n",
       "      <td>1.035827</td>\n",
       "      <td>1.065314</td>\n",
       "      <td>1.531429</td>\n",
       "      <td>2.17858</td>\n",
       "      <td>1.71849</td>\n",
       "      <td>1.702632</td>\n",
       "      <td>...</td>\n",
       "      <td>1.783927</td>\n",
       "      <td>4.309479</td>\n",
       "      <td>2.307944</td>\n",
       "      <td>3.971298</td>\n",
       "      <td>1.534808</td>\n",
       "      <td>4.685414</td>\n",
       "      <td>3.228769</td>\n",
       "      <td>3.805773</td>\n",
       "      <td>1.677267</td>\n",
       "      <td>6705.081965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6271.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24999.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10143.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>49999.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12951.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74999.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16923.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99999.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>52294.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Id             P1             P2             P3  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean    49999.500000       4.088030       4.428085       4.215325   \n",
       "std     28867.657797       2.812963       1.428865       0.842161   \n",
       "min         0.000000       1.000000       1.000000       0.000000   \n",
       "25%     24999.750000       2.000000       3.750000       4.000000   \n",
       "50%     49999.500000       3.000000       5.000000       4.000000   \n",
       "75%     74999.250000       4.000000       5.000000       4.000000   \n",
       "max     99999.000000      15.000000       7.500000       6.000000   \n",
       "\n",
       "                  P4             P5             P6            P7  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.00000   \n",
       "mean        4.396025       1.989590       2.881900       5.30051   \n",
       "std         1.035827       1.065314       1.531429       2.17858   \n",
       "min         2.000000       1.000000       1.000000       1.00000   \n",
       "25%         4.000000       1.000000       2.000000       5.00000   \n",
       "50%         4.000000       2.000000       2.000000       5.00000   \n",
       "75%         5.000000       2.000000       4.000000       5.00000   \n",
       "max         7.500000       6.000000      10.000000      10.00000   \n",
       "\n",
       "                 P8             P9  ...            P29            P30  \\\n",
       "count  100000.00000  100000.000000  ...  100000.000000  100000.000000   \n",
       "mean        4.93100       5.251380  ...       3.084000       2.083300   \n",
       "std         1.71849       1.702632  ...       1.783927       4.309479   \n",
       "min         1.00000       4.000000  ...       0.000000       0.000000   \n",
       "25%         4.00000       4.000000  ...       2.000000       0.000000   \n",
       "50%         5.00000       5.000000  ...       3.000000       0.000000   \n",
       "75%         5.00000       5.000000  ...       3.000000       3.000000   \n",
       "max        10.00000      10.000000  ...      10.000000      25.000000   \n",
       "\n",
       "                 P31            P32            P33            P34  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        1.193330       1.942640       0.987430       2.108670   \n",
       "std         2.307944       3.971298       1.534808       4.685414   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       3.000000       2.000000       3.000000   \n",
       "max        15.000000      25.000000       6.000000      30.000000   \n",
       "\n",
       "                 P35            P36            P37        revenue  \n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000  \n",
       "mean        1.832830       1.968890       0.973500   14698.061620  \n",
       "std         3.228769       3.805773       1.677267    6705.081965  \n",
       "min         0.000000       0.000000       0.000000    6271.000000  \n",
       "25%         0.000000       0.000000       0.000000   10143.000000  \n",
       "50%         0.000000       0.000000       0.000000   12951.000000  \n",
       "75%         4.000000       3.000000       2.000000   16923.000000  \n",
       "max        15.000000      20.000000       8.000000   52294.000000  \n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Open Date'] = pd.to_datetime(df['Open Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Open Date'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Open Date']=df['Open Date'].dt.strftime('%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        01-2011\n",
       "1        03-2011\n",
       "2        10-2013\n",
       "3        05-2013\n",
       "4        07-2013\n",
       "          ...   \n",
       "99995    01-2000\n",
       "99996    07-2011\n",
       "99997    12-2012\n",
       "99998    10-2013\n",
       "99999    10-2010\n",
       "Name: Open Date, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting to just %m-%Y format since we don't consider the day to be relevant\n",
    "df['Open Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_date = df[\"Open Date\"].min()\n",
    "last_date = df[\"Open Date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01-1999'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12-2013'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "İstanbul          34087\n",
       "Ankara             8720\n",
       "İzmir              6465\n",
       "Antalya            5911\n",
       "Kocaeli            4364\n",
       "Mersin             2735\n",
       "Adana              2514\n",
       "Balıkesir          2463\n",
       "Bursa              2441\n",
       "Muğla              1823\n",
       "Aydın              1617\n",
       "Tekirdağ           1577\n",
       "Konya              1576\n",
       "Gaziantep          1487\n",
       "Edirne             1230\n",
       "Manisa             1227\n",
       "Çanakkale           965\n",
       "Denizli             964\n",
       "Diyarbakır          954\n",
       "Hatay               951\n",
       "Zonguldak           926\n",
       "Eskişehir           900\n",
       "Trabzon             660\n",
       "Aksaray             650\n",
       "Bolu                631\n",
       "Yalova              630\n",
       "Kırıkkale           622\n",
       "Malatya             616\n",
       "Mardin              610\n",
       "Şanlıurfa           609\n",
       "Batman              604\n",
       "Sakarya             604\n",
       "Rize                345\n",
       "Artvin              344\n",
       "Bilecik             339\n",
       "Afyonkarahisar      331\n",
       "Nevşehir            328\n",
       "Sivas               326\n",
       "Samsun              324\n",
       "Kayseri             323\n",
       "Kırşehir            319\n",
       "Erzincan            319\n",
       "Erzurum             317\n",
       "Ordu                317\n",
       "Siirt               315\n",
       "Kahramanmaraş       315\n",
       "Niğde               310\n",
       "Giresun             310\n",
       "Çankırı             309\n",
       "Çorum               304\n",
       "Isparta             304\n",
       "Kütahya             304\n",
       "Düzce               303\n",
       "Tanımsız            298\n",
       "Uşak                293\n",
       "Kars                289\n",
       "Kırklareli          281\n",
       "Name: City, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.City.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other         50728\n",
       "Big Cities    49272\n",
       "Name: City Group, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['City Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FC    57019\n",
       "IL    40447\n",
       "DT     2244\n",
       "MB      290\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 1.1 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id            0\n",
       "Open Date     0\n",
       "City          0\n",
       "City Group    0\n",
       "Type          0\n",
       "P1            0\n",
       "P2            0\n",
       "P3            0\n",
       "P4            0\n",
       "P5            0\n",
       "P6            0\n",
       "P7            0\n",
       "P8            0\n",
       "P9            0\n",
       "P10           0\n",
       "P11           0\n",
       "P12           0\n",
       "P13           0\n",
       "P14           0\n",
       "P15           0\n",
       "P16           0\n",
       "P17           0\n",
       "P18           0\n",
       "P19           0\n",
       "P20           0\n",
       "P21           0\n",
       "P22           0\n",
       "P23           0\n",
       "P24           0\n",
       "P25           0\n",
       "P26           0\n",
       "P27           0\n",
       "P28           0\n",
       "P29           0\n",
       "P30           0\n",
       "P31           0\n",
       "P32           0\n",
       "P33           0\n",
       "P34           0\n",
       "P35           0\n",
       "P36           0\n",
       "P37           0\n",
       "revenue       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 - unique values: 9\n",
      "P2 - unique values: 9\n",
      "P3 - unique values: 7\n",
      "P4 - unique values: 7\n",
      "P5 - unique values: 6\n",
      "P6 - unique values: 8\n",
      "P7 - unique values: 7\n",
      "P8 - unique values: 8\n",
      "P9 - unique values: 5\n",
      "P10 - unique values: 4\n",
      "P11 - unique values: 8\n",
      "P12 - unique values: 7\n",
      "P13 - unique values: 5\n",
      "P14 - unique values: 10\n",
      "P15 - unique values: 9\n",
      "P16 - unique values: 9\n",
      "P17 - unique values: 10\n",
      "P18 - unique values: 9\n",
      "P19 - unique values: 9\n",
      "P20 - unique values: 9\n",
      "P21 - unique values: 9\n",
      "P22 - unique values: 5\n",
      "P23 - unique values: 9\n",
      "P24 - unique values: 9\n",
      "P25 - unique values: 9\n",
      "P26 - unique values: 10\n",
      "P27 - unique values: 10\n",
      "P28 - unique values: 9\n",
      "P29 - unique values: 8\n",
      "P30 - unique values: 10\n",
      "P31 - unique values: 10\n",
      "P32 - unique values: 10\n",
      "P33 - unique values: 7\n",
      "P34 - unique values: 11\n",
      "P35 - unique values: 7\n",
      "P36 - unique values: 10\n",
      "P37 - unique values: 8\n"
     ]
    }
   ],
   "source": [
    "#we will take a closer look at columns starting with \"P\" which represent obfuscated data\n",
    "for col in df.columns:\n",
    "    if col[0] == 'P':\n",
    "        print (col, '- unique values:', len(df[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obfuscated_cols = []\n",
    "for col in df.columns:\n",
    "    if col[0] == 'P':\n",
    "        obfuscated_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92414"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~df[obfuscated_cols].all(1)).sum() \n",
    "#out of 100000 rows, 92414 have at least one zero in the obfuscated features P1-037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob = df[obfuscated_cols] #getting a dataframe with just the features starting with \"P\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_obfuscated_cols = df_ob[df_ob == 0].count(axis=0)/df_ob.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P1      0.000\n",
       "P28     0.000\n",
       "P23     0.000\n",
       "P22     0.000\n",
       "P21     0.000\n",
       "P20     0.000\n",
       "P13     0.000\n",
       "P12     0.000\n",
       "P11     0.000\n",
       "P19     0.000\n",
       "P9      0.000\n",
       "P8      0.000\n",
       "P7      0.000\n",
       "P2      0.000\n",
       "P6      0.000\n",
       "P5      0.000\n",
       "P4      0.000\n",
       "P10     0.000\n",
       "P3      0.318\n",
       "P29     3.083\n",
       "P31    65.566\n",
       "P30    65.596\n",
       "P36    65.662\n",
       "P14    65.734\n",
       "P25    65.738\n",
       "P24    65.766\n",
       "P15    65.772\n",
       "P35    65.776\n",
       "P26    65.784\n",
       "P32    65.787\n",
       "P33    65.791\n",
       "P17    65.792\n",
       "P34    65.832\n",
       "P18    65.980\n",
       "P37    66.029\n",
       "P16    66.094\n",
       "P27    66.193\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_obfuscated_cols.sort_values() #getting the missing values (when an instance is zero) for each of the \"P\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P14',\n",
       " 'P15',\n",
       " 'P16',\n",
       " 'P17',\n",
       " 'P18',\n",
       " 'P24',\n",
       " 'P25',\n",
       " 'P26',\n",
       " 'P27',\n",
       " 'P30',\n",
       " 'P31',\n",
       " 'P32',\n",
       " 'P33',\n",
       " 'P34',\n",
       " 'P35',\n",
       " 'P36',\n",
       " 'P37']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping features with more than 65% of missing values\n",
    "missing_obfuscated_cols_to_drop = missing_obfuscated_cols[missing_obfuscated_cols>65].index\n",
    "missing_obfuscated_cols_to_drop = list(missing_obfuscated_cols_to_drop)\n",
    "missing_obfuscated_cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(missing_obfuscated_cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing missing values of P3 and P29 with the mean\n",
    "df['P3']=df['P3'].replace(0,df['P3'].mean())\n",
    "df['P29']=df['P3'].replace(0,df['P29'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P1, P2 - P37**: There are three categories of these obfuscated data:\n",
    "- Demographic data (population in any given area, age and gender distribution, development scales)\n",
    "- Real estate data (m2 of the location, front facade of the location, car park availability)\n",
    "- Commercial data (points of interest including schools, banks, other QSR operators)\n",
    "\n",
    "Although we know that there are 3 categories in the obfuscated data represented from columns P1-P37, we are not able to choose an imputation method at the risk of changing the original data. A zero could represent that there are no points of interest in a given city or simply they were not measured, thus it would be wrong for us to input the missing values with the mean or median for that certain feature. We have chosen to drop from our analysis features whose missing values represent more than 65%, those are: P31, P30, P36, P14, P25, P24, P15, P35, P26, P32, P33, P17, P34, P18, P37, P16 and P27. As such, we will end up with only 2 of the obfuscated features with missing values, P3 and P29, with zeros representing 0.3% and 3.1%. Given these low percetages, we will replace the zeros with the mean. \n",
    "\n",
    "One call out about these obfuscated features, whose instances seem to between 1 and 25 could be that, for example, they are the order of the answers in a formulaire e.g. for question 1 (P1), Restaurant with ID 1 has answered the first option 1. We should again, keep in mind, we are just making this assumption. \n",
    "This would highlight the fact that this representation may cause some issues since some Machine Learning algorithms will assume that two nearby values (answering option 1 and option 2) are more similar than two distant (answering option 1 and option 25). It may not be the case for the obfuscated features. To fix this, we will create one binary attribute per obfuscated feature: one attribute equal to 1 when P1 is \"1\" (and 0 otherwise). We will also use One Hot Encoding for the categorical features we have analysed: City, City Group and Type as well as the time series data (Open Date). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.2 Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 1.2.a) Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.drop(['Id','revenue'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder()\n",
    "df_cat_1hot = cat_encoder.fit_transform(df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 350)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat_1hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_01-1999', 'x0_01-2000', 'x0_01-2005', 'x0_01-2009',\n",
       "       'x0_01-2011', 'x0_01-2012', 'x0_01-2013', 'x0_01-2014',\n",
       "       'x0_02-1998', 'x0_02-2000', 'x0_02-2004', 'x0_02-2007',\n",
       "       'x0_02-2009', 'x0_02-2010', 'x0_02-2011', 'x0_02-2012',\n",
       "       'x0_02-2013', 'x0_03-1996', 'x0_03-1998', 'x0_03-2002',\n",
       "       'x0_03-2005', 'x0_03-2006', 'x0_03-2007', 'x0_03-2008',\n",
       "       'x0_03-2009', 'x0_03-2010', 'x0_03-2011', 'x0_03-2012',\n",
       "       'x0_03-2013', 'x0_04-1997', 'x0_04-1998', 'x0_04-2000',\n",
       "       'x0_04-2006', 'x0_04-2007', 'x0_04-2008', 'x0_04-2009',\n",
       "       'x0_04-2010', 'x0_04-2011', 'x0_04-2012', 'x0_04-2013',\n",
       "       'x0_05-1995', 'x0_05-1997', 'x0_05-1998', 'x0_05-2006',\n",
       "       'x0_05-2007', 'x0_05-2008', 'x0_05-2009', 'x0_05-2010',\n",
       "       'x0_05-2011', 'x0_05-2012', 'x0_05-2013', 'x0_06-1996',\n",
       "       'x0_06-1997', 'x0_06-1999', 'x0_06-2000', 'x0_06-2001',\n",
       "       'x0_06-2003', 'x0_06-2006', 'x0_06-2007', 'x0_06-2009',\n",
       "       'x0_06-2010', 'x0_06-2011', 'x0_06-2012', 'x0_06-2013',\n",
       "       'x0_07-1997', 'x0_07-2000', 'x0_07-2004', 'x0_07-2007',\n",
       "       'x0_07-2008', 'x0_07-2009', 'x0_07-2010', 'x0_07-2011',\n",
       "       'x0_07-2012', 'x0_07-2013', 'x0_08-1995', 'x0_08-1996',\n",
       "       'x0_08-1999', 'x0_08-2000', 'x0_08-2004', 'x0_08-2005',\n",
       "       'x0_08-2006', 'x0_08-2007', 'x0_08-2008', 'x0_08-2009',\n",
       "       'x0_08-2010', 'x0_08-2011', 'x0_08-2012', 'x0_08-2013',\n",
       "       'x0_09-1996', 'x0_09-1997', 'x0_09-1998', 'x0_09-1999',\n",
       "       'x0_09-2001', 'x0_09-2003', 'x0_09-2005', 'x0_09-2006',\n",
       "       'x0_09-2007', 'x0_09-2008', 'x0_09-2009', 'x0_09-2010',\n",
       "       'x0_09-2011', 'x0_09-2012', 'x0_09-2013', 'x0_10-1995',\n",
       "       'x0_10-1999', 'x0_10-2000', 'x0_10-2004', 'x0_10-2006',\n",
       "       'x0_10-2007', 'x0_10-2008', 'x0_10-2009', 'x0_10-2010',\n",
       "       'x0_10-2011', 'x0_10-2012', 'x0_10-2013', 'x0_11-1995',\n",
       "       'x0_11-1999', 'x0_11-2000', 'x0_11-2006', 'x0_11-2007',\n",
       "       'x0_11-2008', 'x0_11-2009', 'x0_11-2010', 'x0_11-2011',\n",
       "       'x0_11-2012', 'x0_11-2013', 'x0_12-1997', 'x0_12-1998',\n",
       "       'x0_12-1999', 'x0_12-2003', 'x0_12-2004', 'x0_12-2005',\n",
       "       'x0_12-2006', 'x0_12-2007', 'x0_12-2008', 'x0_12-2009',\n",
       "       'x0_12-2010', 'x0_12-2011', 'x0_12-2012', 'x0_12-2013', 'x1_Adana',\n",
       "       'x1_Afyonkarahisar', 'x1_Aksaray', 'x1_Ankara', 'x1_Antalya',\n",
       "       'x1_Artvin', 'x1_Aydın', 'x1_Balıkesir', 'x1_Batman', 'x1_Bilecik',\n",
       "       'x1_Bolu', 'x1_Bursa', 'x1_Denizli', 'x1_Diyarbakır', 'x1_Düzce',\n",
       "       'x1_Edirne', 'x1_Erzincan', 'x1_Erzurum', 'x1_Eskişehir',\n",
       "       'x1_Gaziantep', 'x1_Giresun', 'x1_Hatay', 'x1_Isparta',\n",
       "       'x1_Kahramanmaraş', 'x1_Kars', 'x1_Kayseri', 'x1_Kocaeli',\n",
       "       'x1_Konya', 'x1_Kütahya', 'x1_Kırklareli', 'x1_Kırıkkale',\n",
       "       'x1_Kırşehir', 'x1_Malatya', 'x1_Manisa', 'x1_Mardin', 'x1_Mersin',\n",
       "       'x1_Muğla', 'x1_Nevşehir', 'x1_Niğde', 'x1_Ordu', 'x1_Rize',\n",
       "       'x1_Sakarya', 'x1_Samsun', 'x1_Siirt', 'x1_Sivas', 'x1_Tanımsız',\n",
       "       'x1_Tekirdağ', 'x1_Trabzon', 'x1_Uşak', 'x1_Yalova',\n",
       "       'x1_Zonguldak', 'x1_Çanakkale', 'x1_Çankırı', 'x1_Çorum',\n",
       "       'x1_İstanbul', 'x1_İzmir', 'x1_Şanlıurfa', 'x2_Big Cities',\n",
       "       'x2_Other', 'x3_DT', 'x3_FC', 'x3_IL', 'x3_MB', 'x4_1', 'x4_2',\n",
       "       'x4_3', 'x4_4', 'x4_5', 'x4_6', 'x4_9', 'x4_12', 'x4_15', 'x5_1.0',\n",
       "       'x5_1.5', 'x5_2.0', 'x5_3.0', 'x5_4.0', 'x5_4.5', 'x5_5.0',\n",
       "       'x5_6.0', 'x5_7.5', 'x6_2.0', 'x6_3.0', 'x6_4.0', 'x6_4.215325',\n",
       "       'x6_4.5', 'x6_5.0', 'x6_6.0', 'x7_2.0', 'x7_3.0', 'x7_4.0',\n",
       "       'x7_4.5', 'x7_5.0', 'x7_6.0', 'x7_7.5', 'x8_1', 'x8_2', 'x8_3',\n",
       "       'x8_4', 'x8_5', 'x8_6', 'x9_1', 'x9_2', 'x9_3', 'x9_4', 'x9_5',\n",
       "       'x9_6', 'x9_8', 'x9_10', 'x10_1', 'x10_2', 'x10_3', 'x10_4',\n",
       "       'x10_5', 'x10_6', 'x10_10', 'x11_1', 'x11_2', 'x11_3', 'x11_4',\n",
       "       'x11_5', 'x11_6', 'x11_8', 'x11_10', 'x12_4', 'x12_5', 'x12_6',\n",
       "       'x12_8', 'x12_10', 'x13_4', 'x13_5', 'x13_8', 'x13_10', 'x14_1',\n",
       "       'x14_2', 'x14_3', 'x14_4', 'x14_5', 'x14_6', 'x14_8', 'x14_10',\n",
       "       'x15_2', 'x15_3', 'x15_4', 'x15_5', 'x15_6', 'x15_8', 'x15_10',\n",
       "       'x16_3.0', 'x16_4.0', 'x16_5.0', 'x16_6.0', 'x16_7.5', 'x17_1',\n",
       "       'x17_2', 'x17_3', 'x17_4', 'x17_5', 'x17_10', 'x17_15', 'x17_20',\n",
       "       'x17_25', 'x18_1', 'x18_2', 'x18_3', 'x18_4', 'x18_5', 'x18_6',\n",
       "       'x18_9', 'x18_12', 'x18_15', 'x19_1', 'x19_2', 'x19_3', 'x19_4',\n",
       "       'x19_5', 'x19_6', 'x19_9', 'x19_12', 'x19_15', 'x20_1', 'x20_2',\n",
       "       'x20_3', 'x20_4', 'x20_5', 'x21_1', 'x21_2', 'x21_3', 'x21_4',\n",
       "       'x21_5', 'x21_10', 'x21_15', 'x21_20', 'x21_25', 'x22_1.0',\n",
       "       'x22_2.0', 'x22_2.5', 'x22_3.0', 'x22_4.0', 'x22_5.0', 'x22_7.5',\n",
       "       'x22_10.0', 'x22_12.5', 'x23_2.0', 'x23_3.0', 'x23_4.0',\n",
       "       'x23_4.215325', 'x23_4.5', 'x23_5.0', 'x23_6.0'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_encoder.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df_cat_1hot.toarray(), columns=cat_encoder.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_01-1999</th>\n",
       "      <th>x0_01-2000</th>\n",
       "      <th>x0_01-2005</th>\n",
       "      <th>x0_01-2009</th>\n",
       "      <th>x0_01-2011</th>\n",
       "      <th>x0_01-2012</th>\n",
       "      <th>x0_01-2013</th>\n",
       "      <th>x0_01-2014</th>\n",
       "      <th>x0_02-1998</th>\n",
       "      <th>x0_02-2000</th>\n",
       "      <th>...</th>\n",
       "      <th>x22_7.5</th>\n",
       "      <th>x22_10.0</th>\n",
       "      <th>x22_12.5</th>\n",
       "      <th>x23_2.0</th>\n",
       "      <th>x23_3.0</th>\n",
       "      <th>x23_4.0</th>\n",
       "      <th>x23_4.215325</th>\n",
       "      <th>x23_4.5</th>\n",
       "      <th>x23_5.0</th>\n",
       "      <th>x23_6.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0_01-1999  x0_01-2000  x0_01-2005  x0_01-2009  x0_01-2011  x0_01-2012  \\\n",
       "0         0.0         0.0         0.0         0.0         1.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   x0_01-2013  x0_01-2014  x0_02-1998  x0_02-2000  ...  x22_7.5  x22_10.0  \\\n",
       "0         0.0         0.0         0.0         0.0  ...      0.0       0.0   \n",
       "1         0.0         0.0         0.0         0.0  ...      0.0       0.0   \n",
       "\n",
       "   x22_12.5  x23_2.0  x23_3.0  x23_4.0  x23_4.215325  x23_4.5  x23_5.0  \\\n",
       "0       0.0      0.0      0.0      1.0           0.0      0.0      0.0   \n",
       "1       0.0      0.0      0.0      1.0           0.0      0.0      0.0   \n",
       "\n",
       "   x23_6.0  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "\n",
       "[2 rows x 350 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 1.3 Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revenue: The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. Please note that the values are transformed so they don't mean real dollar values.\n",
    "\n",
    "RevenueCategory - the revenue category, where price can be below 12000 (\"<12K\"), between 12000 and 20000 (\"12K-20K\"), or above 20000 (\">20K\"). This is the target variable that you're trying to predict in the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.revenue.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52294.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.revenue.max() #to get the last value for the bin numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['revenue_category'] = pd.cut(df.revenue,\n",
    "                               bins=[0, 12000, 20000, 52295], \n",
    "                                right=False,\n",
    "                                labels = ['<12K','12K-20K','>20K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>revenue_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12K-20K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12K-20K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      revenue revenue_category\n",
       "1389  12000.0          12K-20K\n",
       "1471  12000.0          12K-20K"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.revenue==12000][['revenue','revenue_category']].head(2) #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>revenue_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74427</th>\n",
       "      <td>52294.0</td>\n",
       "      <td>&gt;20K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       revenue revenue_category\n",
       "74427  52294.0             >20K"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.revenue==df.revenue.max()][['revenue','revenue_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12K-20K</th>\n",
       "      <td>43.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;12K</th>\n",
       "      <td>43.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;20K</th>\n",
       "      <td>12.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         revenue_category\n",
       "12K-20K             43.94\n",
       "<12K                43.40\n",
       ">20K                12.66"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.round(df['revenue_category'].value_counts(normalize=True) * 100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Open Date</th>\n",
       "      <th>City</th>\n",
       "      <th>City Group</th>\n",
       "      <th>Type</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>...</th>\n",
       "      <th>P13</th>\n",
       "      <th>P19</th>\n",
       "      <th>P20</th>\n",
       "      <th>P21</th>\n",
       "      <th>P22</th>\n",
       "      <th>P23</th>\n",
       "      <th>P28</th>\n",
       "      <th>P29</th>\n",
       "      <th>revenue</th>\n",
       "      <th>revenue_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Open Date, City, City Group, Type, P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P19, P20, P21, P22, P23, P28, P29, revenue, revenue_category]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['revenue_category'].isnull().values] #checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['revenue_category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will apply the stratified sampling method to ensure that the right number of instances are sampled from each of the 3 categories (<12K, 12K-20K, >20K) such that the test set is representative of the overall population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_cat, y_test_cat = train_test_split(X, df['revenue_category'], test_size = 0.2, stratify=df['revenue_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape is :  (80000, 350)\n",
      "X_test shape  is :  (20000, 350)\n",
      "y_train shape is :  (80000,)\n",
      "y_test shape is :  (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape is : \", X_train.shape)\n",
    "print(\"X_test shape  is : \", X_test.shape)\n",
    "print(\"y_train shape is : \", y_train_cat.shape)\n",
    "print(\"y_test shape is : \", y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12K-20K</th>\n",
       "      <td>43.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;12K</th>\n",
       "      <td>43.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;20K</th>\n",
       "      <td>12.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         revenue_category\n",
       "12K-20K             43.94\n",
       "<12K                43.40\n",
       ">20K                12.66"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.round(y_test_cat.value_counts(normalize=True) * 100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rev, X_test_rev, y_train_rev, y_test_rev = train_test_split(X, df['revenue'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_rev shape is :  (80000, 350)\n",
      "X_test_rev shape  is :  (20000, 350)\n",
      "y_train_rev shape is :  (80000,)\n",
      "y_test_rev shape is :  (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_rev shape is : \", X_train_rev.shape)\n",
    "print(\"X_test_rev shape  is : \", X_test_rev.shape)\n",
    "print(\"y_train_rev shape is : \", y_train_rev.shape)\n",
    "print(\"y_test_rev shape is : \", y_test_rev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The test set generated using stratified sampling has revenue category proportions similar to those in the full dataset we've previously seen above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Simple Classifiers\n",
    "\n",
    "* Choose **`X` classifiers** (https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).\n",
    "* Use **grid-search and stratified 10 fold cross-validation** to estimate the best parameters (https://scikit-learn.org/stable/model_selection.html#model-selection). \n",
    "* Present mean and standard deviation of accuracy, precision and recall.\n",
    "* Show confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ['accuracy','precision','recall'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = StratifiedKFold(10) #using stratified 10 fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 2.1. Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters= dict(max_iter=[80,100,120], \n",
    "                       C=[0.5,1,2,3], \n",
    "                       penalty=['l1', 'l2', 'elasticnet', 'none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters found on development set:\n",
      "\n",
      "{'C': 2, 'max_iter': 80, 'penalty': 'l1'}\n",
      "\n",
      "Grid accuracy scores on development set:\n",
      "\n",
      "  Mean = 0.360 | Std Deviation = 0.392 for {'C': 0.5, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = 0.540 | Std Deviation = 0.475 for {'C': 0.5, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.500 | Std Deviation = 0.410 for {'C': 0.5, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = 0.360 | Std Deviation = 0.392 for {'C': 0.5, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = 0.540 | Std Deviation = 0.475 for {'C': 0.5, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.520 | Std Deviation = 0.445 for {'C': 0.5, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = 0.360 | Std Deviation = 0.392 for {'C': 0.5, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = 0.540 | Std Deviation = 0.475 for {'C': 0.5, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.540 | Std Deviation = 0.312 for {'C': 0.5, 'max_iter': 120, 'penalty': 'none'}\n",
      "  Mean = 0.460 | Std Deviation = 0.360 for {'C': 1, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = 0.520 | Std Deviation = 0.445 for {'C': 1, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.560 | Std Deviation = 0.431 for {'C': 1, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = 0.460 | Std Deviation = 0.360 for {'C': 1, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = 0.520 | Std Deviation = 0.445 for {'C': 1, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.500 | Std Deviation = 0.482 for {'C': 1, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = 0.460 | Std Deviation = 0.360 for {'C': 1, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = 0.520 | Std Deviation = 0.445 for {'C': 1, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.480 | Std Deviation = 0.445 for {'C': 1, 'max_iter': 120, 'penalty': 'none'}\n",
      "  Mean = 0.600 | Std Deviation = 0.253 for {'C': 2, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = 0.520 | Std Deviation = 0.445 for {'C': 2, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.540 | Std Deviation = 0.402 for {'C': 2, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = 0.580 | Std Deviation = 0.280 for {'C': 2, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = 0.520 | Std Deviation = 0.445 for {'C': 2, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.540 | Std Deviation = 0.475 for {'C': 2, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = 0.560 | Std Deviation = 0.299 for {'C': 2, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = 0.520 | Std Deviation = 0.445 for {'C': 2, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.500 | Std Deviation = 0.482 for {'C': 2, 'max_iter': 120, 'penalty': 'none'}\n",
      "  Mean = 0.560 | Std Deviation = 0.299 for {'C': 3, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = 0.520 | Std Deviation = 0.445 for {'C': 3, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.520 | Std Deviation = 0.367 for {'C': 3, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = 0.560 | Std Deviation = 0.299 for {'C': 3, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = 0.520 | Std Deviation = 0.445 for {'C': 3, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.500 | Std Deviation = 0.482 for {'C': 3, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = 0.560 | Std Deviation = 0.299 for {'C': 3, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = 0.520 | Std Deviation = 0.445 for {'C': 3, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = 0.520 | Std Deviation = 0.445 for {'C': 3, 'max_iter': 120, 'penalty': 'none'}\n",
      "\n",
      "Confusion matrix based on the full evaluation set:\n",
      "\n",
      "[[ 7  6  1]\n",
      " [10 20  0]\n",
      " [ 2  0  4]]\n",
      "\n",
      "Accuracy 0.62\n",
      "Recall 0.62\n",
      "Precision 0.661\n",
      "______________________________________\n",
      "\n",
      "Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters found on development set:\n",
      "\n",
      "{'C': 0.5, 'max_iter': 80, 'penalty': 'l1'}\n",
      "\n",
      "Grid precision scores on development set:\n",
      "\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 120, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 120, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 120, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 120, 'penalty': 'none'}\n",
      "\n",
      "Confusion matrix based on the full evaluation set:\n",
      "\n",
      "[[ 7  6  1]\n",
      " [14 16  0]\n",
      " [ 1  1  4]]\n",
      "\n",
      "Accuracy 0.54\n",
      "Recall 0.54\n",
      "Precision 0.602\n",
      "______________________________________\n",
      "\n",
      "Tuning hyper-parameters for recall\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found on development set:\n",
      "\n",
      "{'C': 0.5, 'max_iter': 80, 'penalty': 'l1'}\n",
      "\n",
      "Grid recall scores on development set:\n",
      "\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 0.5, 'max_iter': 120, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'max_iter': 120, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 2, 'max_iter': 120, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 80, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 80, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 80, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 80, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 100, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 100, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 100, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 100, 'penalty': 'none'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 120, 'penalty': 'l1'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 120, 'penalty': 'l2'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 120, 'penalty': 'elasticnet'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 3, 'max_iter': 120, 'penalty': 'none'}\n",
      "\n",
      "Confusion matrix based on the full evaluation set:\n",
      "\n",
      "[[ 7  6  1]\n",
      " [14 16  0]\n",
      " [ 1  1  4]]\n",
      "\n",
      "Accuracy 0.54\n",
      "Recall 0.54\n",
      "Precision 0.602\n",
      "______________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score in scores: #optimizing for each of the performance metrics\n",
    "    print(\"Tuning hyper-parameters for\", score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(LogisticRegression(solver='saga'), #using gridsearch\n",
    "                       tuned_parameters, \n",
    "                       scoring=score, \n",
    "                       cv=kfolds.split(X_train[:50], y_train_cat[:50]))\n",
    "    clf.fit(X_train[:50], y_train_cat[:50])\n",
    "\n",
    "    print(\"Best parameters found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid\", score,\"scores on development set:\") \n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']): #we will print out the mean and standard deviation\n",
    "        print(\"  Mean = %0.3f | Std Deviation = %0.3f for %r\"\n",
    "              % (mean, std * 2, params)) #for each parameter combination\n",
    "    print()\n",
    "    print(\"Confusion matrix based on the full evaluation set:\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test_cat[:50], clf.predict(X_test[:50])\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print()\n",
    "    print('Accuracy', np.round(accuracy_score(y_true,y_pred),3))\n",
    "    print('Recall', np.round(recall_score(y_true,y_pred, average='weighted'),3))\n",
    "    print('Precision', np.round(precision_score(y_true,y_pred, average='weighted'),3))\n",
    "    print(\"______________________________________\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 2.2. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters= dict(criterion=['gini','entropy'],\n",
    "                       max_depth= np.arange(3, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters found on development set:\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 4}\n",
      "\n",
      "Grid accuracy scores on development set:\n",
      "\n",
      "  Mean = 0.440 | Std Deviation = 0.299 for {'criterion': 'gini', 'max_depth': 3}\n",
      "  Mean = 0.520 | Std Deviation = 0.480 for {'criterion': 'gini', 'max_depth': 4}\n",
      "  Mean = 0.440 | Std Deviation = 0.349 for {'criterion': 'gini', 'max_depth': 5}\n",
      "  Mean = 0.440 | Std Deviation = 0.531 for {'criterion': 'gini', 'max_depth': 6}\n",
      "  Mean = 0.440 | Std Deviation = 0.431 for {'criterion': 'gini', 'max_depth': 7}\n",
      "  Mean = 0.440 | Std Deviation = 0.500 for {'criterion': 'gini', 'max_depth': 8}\n",
      "  Mean = 0.480 | Std Deviation = 0.599 for {'criterion': 'gini', 'max_depth': 9}\n",
      "  Mean = 0.420 | Std Deviation = 0.454 for {'criterion': 'gini', 'max_depth': 10}\n",
      "  Mean = 0.480 | Std Deviation = 0.445 for {'criterion': 'gini', 'max_depth': 11}\n",
      "  Mean = 0.380 | Std Deviation = 0.418 for {'criterion': 'gini', 'max_depth': 12}\n",
      "  Mean = 0.380 | Std Deviation = 0.418 for {'criterion': 'gini', 'max_depth': 13}\n",
      "  Mean = 0.480 | Std Deviation = 0.543 for {'criterion': 'gini', 'max_depth': 14}\n",
      "  Mean = 0.480 | Std Deviation = 0.265 for {'criterion': 'entropy', 'max_depth': 3}\n",
      "  Mean = 0.460 | Std Deviation = 0.440 for {'criterion': 'entropy', 'max_depth': 4}\n",
      "  Mean = 0.480 | Std Deviation = 0.367 for {'criterion': 'entropy', 'max_depth': 5}\n",
      "  Mean = 0.400 | Std Deviation = 0.506 for {'criterion': 'entropy', 'max_depth': 6}\n",
      "  Mean = 0.380 | Std Deviation = 0.418 for {'criterion': 'entropy', 'max_depth': 7}\n",
      "  Mean = 0.380 | Std Deviation = 0.550 for {'criterion': 'entropy', 'max_depth': 8}\n",
      "  Mean = 0.400 | Std Deviation = 0.537 for {'criterion': 'entropy', 'max_depth': 9}\n",
      "  Mean = 0.440 | Std Deviation = 0.392 for {'criterion': 'entropy', 'max_depth': 10}\n",
      "  Mean = 0.340 | Std Deviation = 0.475 for {'criterion': 'entropy', 'max_depth': 11}\n",
      "  Mean = 0.320 | Std Deviation = 0.480 for {'criterion': 'entropy', 'max_depth': 12}\n",
      "  Mean = 0.420 | Std Deviation = 0.488 for {'criterion': 'entropy', 'max_depth': 13}\n",
      "  Mean = 0.380 | Std Deviation = 0.418 for {'criterion': 'entropy', 'max_depth': 14}\n",
      "\n",
      "Confusion matrix based on the full evaluation set:\n",
      "\n",
      "[[11  3  0]\n",
      " [15 14  1]\n",
      " [ 3  1  2]]\n",
      "\n",
      "Accuracy 0.54\n",
      "Recall 0.54\n",
      "Precision 0.653\n",
      "______________________________________\n",
      "\n",
      "Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters found on development set:\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 3}\n",
      "\n",
      "Grid precision scores on development set:\n",
      "\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 3}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 4}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 5}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 6}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 7}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 8}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 9}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 10}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 11}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 12}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 13}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 14}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 3}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 4}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 5}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 6}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 7}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 8}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 9}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 10}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 11}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 12}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 13}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 14}\n",
      "\n",
      "Confusion matrix based on the full evaluation set:\n",
      "\n",
      "[[11  3  0]\n",
      " [17 12  1]\n",
      " [ 3  1  2]]\n",
      "\n",
      "Accuracy 0.5\n",
      "Recall 0.5\n",
      "Precision 0.629\n",
      "______________________________________\n",
      "\n",
      "Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters found on development set:\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 3}\n",
      "\n",
      "Grid recall scores on development set:\n",
      "\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 3}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 4}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 5}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 6}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 7}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 8}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 9}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 10}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 11}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 12}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 13}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'gini', 'max_depth': 14}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 3}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 4}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 5}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 6}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 7}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 8}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 9}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 10}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 11}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 12}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 13}\n",
      "  Mean = nan | Std Deviation = nan for {'criterion': 'entropy', 'max_depth': 14}\n",
      "\n",
      "Confusion matrix based on the full evaluation set:\n",
      "\n",
      "[[11  3  0]\n",
      " [17 12  1]\n",
      " [ 3  1  2]]\n",
      "\n",
      "Accuracy 0.5\n",
      "Recall 0.5\n",
      "Precision 0.629\n",
      "______________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score in scores:\n",
    "    print(\"Tuning hyper-parameters for\", score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        DecisionTreeClassifier(), \n",
    "        tuned_parameters, \n",
    "        scoring=score,\n",
    "        cv=kfolds.split(X_train[:50], y_train_cat[:50]))\n",
    "    \n",
    "    clf.fit(X_train[:50], y_train_cat[:50])\n",
    "\n",
    "    print(\"Best parameters found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid\", score,\"scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"  Mean = %0.3f | Std Deviation = %0.3f for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Confusion matrix based on the full evaluation set:\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test_cat[:50], clf.predict(X_test[:50])\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print()\n",
    "    print('Accuracy', np.round(accuracy_score(y_true,y_pred),3))\n",
    "    print('Recall', np.round(recall_score(y_true,y_pred, average='weighted'),3))\n",
    "    print('Precision', np.round(precision_score(y_true,y_pred, average='weighted'),3))\n",
    "    print(\"______________________________________\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 1.3. SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters= dict(kernel=['rbf','linear'], \n",
    "                       gamma= [1e-3, 1e-4],\n",
    "                       C= [1, 10, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters found on development set:\n",
      "\n",
      "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid accuracy scores on development set:\n",
      "\n",
      "  Mean = 0.400 | Std Deviation = 0.000 for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "  Mean = 0.540 | Std Deviation = 0.402 for {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "  Mean = 0.400 | Std Deviation = 0.000 for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "  Mean = 0.540 | Std Deviation = 0.402 for {'C': 1, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "  Mean = 0.460 | Std Deviation = 0.183 for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "  Mean = 0.540 | Std Deviation = 0.402 for {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "  Mean = 0.400 | Std Deviation = 0.000 for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "  Mean = 0.540 | Std Deviation = 0.402 for {'C': 10, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "  Mean = 0.580 | Std Deviation = 0.418 for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "  Mean = 0.540 | Std Deviation = 0.402 for {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "  Mean = 0.460 | Std Deviation = 0.183 for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "  Mean = 0.540 | Std Deviation = 0.402 for {'C': 100, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "\n",
      "Confusion matrix based on the full evaluation set:\n",
      "\n",
      "[[ 6  7  1]\n",
      " [ 7 23  0]\n",
      " [ 2  0  4]]\n",
      "\n",
      "Accuracy 0.66\n",
      "Recall 0.66\n",
      "Precision 0.668\n",
      "______________________________________\n",
      "\n",
      "Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters found on development set:\n",
      "\n",
      "{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid precision scores on development set:\n",
      "\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 10, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 100, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "\n",
      "Confusion matrix based on the full evaluation set:\n",
      "\n",
      "[[ 0 14  0]\n",
      " [ 0 30  0]\n",
      " [ 0  6  0]]\n",
      "\n",
      "Accuracy 0.6\n",
      "Recall 0.6\n",
      "Precision 0.36\n",
      "______________________________________\n",
      "\n",
      "Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters found on development set:\n",
      "\n",
      "{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid recall scores on development set:\n",
      "\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 1, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 10, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "  Mean = nan | Std Deviation = nan for {'C': 100, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "\n",
      "Confusion matrix based on the full evaluation set:\n",
      "\n",
      "[[ 0 14  0]\n",
      " [ 0 30  0]\n",
      " [ 0  6  0]]\n",
      "\n",
      "Accuracy 0.6\n",
      "Recall 0.6\n",
      "Precision 0.36\n",
      "______________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score in scores:\n",
    "    print(\"Tuning hyper-parameters for\", score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), \n",
    "                       tuned_parameters, \n",
    "                       scoring=score,\n",
    "                       cv=kfolds.split(X_train[:50], y_train_cat[:50]))\n",
    "    clf.fit(X_train[:50], y_train_cat[:50])\n",
    "\n",
    "    print(\"Best parameters found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid\", score,\"scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"  Mean = %0.3f | Std Deviation = %0.3f for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Confusion matrix based on the full evaluation set:\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test_cat[:50], clf.predict(X_test[:50])\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print()\n",
    "    print('Accuracy', np.round(accuracy_score(y_true,y_pred),3))\n",
    "    print('Recall', np.round(recall_score(y_true,y_pred, average='weighted'),3))\n",
    "    print('Precision', np.round(precision_score(y_true,y_pred, average='weighted'),3))\n",
    "    print(\"______________________________________\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Simple Regressors\n",
    "\n",
    "* Choose **`X` regressors** (https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).\n",
    "* Use **grid-search and 10 fold cross-validation** to estimate the best parameters (https://scikit-learn.org/stable/model_selection.html#model-selection). \n",
    "* Use the mean absolute error regression loss, or other relevant metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present and Discuss your results here\n",
    "# ...\n",
    "#linear regressor, svm, decision tree\n",
    "#RMSE (root mean squared error),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 3.1. Lasso Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters= dict( alpha = [0,0.5,1.0],\n",
    "                       max_iter = [200,500,1000],\n",
    "                       selection = [\"cyclic\", \"random\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 1.0, 'max_iter': 200, 'selection': 'cyclic'}\n",
      "\n",
      "Best score: -3588.15508866543\n"
     ]
    }
   ],
   "source": [
    "rg = GridSearchCV(linear_model.Lasso(), \n",
    "                   tuned_parameters,\n",
    "                   cv=3,\n",
    "                   scoring='neg_mean_absolute_error')\n",
    "\n",
    "rg.fit(X_train_rev[:50], y_train_rev[:50])\n",
    "\n",
    "print('Best parameters:', rg.best_params_)\n",
    "print()\n",
    "print('Best score:', rg.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 3.2. SVM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters= dict(kernel = ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                       C = [1,2,3],\n",
    "                       degree = [3,4,5,6,7,8],\n",
    "                       gamma = ['auto','scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 3, 'degree': 3, 'gamma': 'auto', 'kernel': 'linear'}\n",
      "\n",
      "Best score: -4943.449333333333\n"
     ]
    }
   ],
   "source": [
    "rg = GridSearchCV(SVR(), \n",
    "                   tuned_parameters,\n",
    "                   cv=3,\n",
    "                   scoring='neg_mean_absolute_error')\n",
    "\n",
    "rg.fit(X_train_rev[:150], y_train_rev[:150])\n",
    "\n",
    "print('Best parameters:', rg.best_params_)\n",
    "print()\n",
    "print('Best score:', rg.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 3.3. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters= dict( max_depth = [2,4,6,8,10],\n",
    "                      min_samples_split = [2,4,6,8,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
      "\n",
      "Best score: -5352.010161842514\n"
     ]
    }
   ],
   "source": [
    "rg = GridSearchCV(DecisionTreeRegressor(), \n",
    "                   tuned_parameters,\n",
    "                   cv=3,\n",
    "                   scoring='neg_mean_absolute_error')\n",
    "\n",
    "rg.fit(X_train_rev[:50], y_train_rev[:50])\n",
    "\n",
    "print('Best parameters:', rg.best_params_)\n",
    "print()\n",
    "print('Best score:', rg.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier/Regressor\n",
    "\n",
    "* Use a voting classifier (http://scikit-learn.org/stable/modules/ensemble.html#voting-classifier)/regressor(https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html) to combine the best results of the `X` classifiers/regressors from previous sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, VotingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 4.1. Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Classification Voting Ensemble the predictions are the majority vote of contributing models. We will explore the hard voting method which involves summing the predictions for each class label and predicting the class label with the most votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(solver='saga')\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_class_hard = VotingClassifier(estimators=[('logr', clf1), \n",
    "                                     ('rf', clf2), \n",
    "                                     ('svc', clf3)], \n",
    "                                     voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_class_hard = voting_class_hard.fit(X_train[:50], y_train_cat[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "\n",
      "[[ 5  8  1]\n",
      " [12 11  7]\n",
      " [ 2  2  2]]\n",
      "\n",
      "Accuracy 0.36\n",
      "Recall 0.36\n",
      "Precision 0.412\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "print()\n",
    "y_true, y_pred = y_test_cat[:50], voting_class_hard.predict(X_train[:50])\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print()\n",
    "print('Accuracy', np.round(accuracy_score(y_true,y_pred),3))\n",
    "print('Recall', np.round(recall_score(y_true,y_pred, average='weighted'),3))\n",
    "print('Precision', np.round(precision_score(y_true,y_pred, average='weighted'),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 4.2. Voting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the simplest ensemble method which is voting to regression models we've seen in exercise 3. We simply average the predictions from the different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg1 = linear_model.Lasso()\n",
    "rg2 = SVR()\n",
    "rg3 = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_rg = VotingRegressor(estimators=[('lasso',rg1), \n",
    "                                     ('svr', rg2), \n",
    "                                     ('dtrg', rg3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_rg = voting_rg.fit(X_train_rev[:50], y_train_rev[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n"
     ]
    }
   ],
   "source": [
    "print('Best score:', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3366135955178813"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_rg.score(X_test_rev[:50], y_test_rev[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble model has a lower MSE than either model individually, even though it simply averages the predictions from the two models. This example illustrates the power of ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost \n",
    "\n",
    "* Use [XGBoost](https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.datacamp.com/community/tutorials/xgboost-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.4.2-py3-none-win_amd64.whl (97.8 MB)\n",
      "Requirement already satisfied: scipy in d:\\apps\\anaconda\\lib\\site-packages (from xgboost) (1.6.1)\n",
      "Requirement already satisfied: numpy in d:\\apps\\anaconda\\lib\\site-packages (from xgboost) (1.19.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "* Use [Random Forests](http://scikit-learn.org/stable/modules/ensemble.html#random-forests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {'n_estimators'      : [100,330,340],\n",
    "                'max_depth'         : [8, 9, 10, 11, 12],\n",
    "                'random_state'      : [42]}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in scores:\n",
    "    print(\"Tuning hyper-parameters for\", score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(), \n",
    "                       tuned_parameters, \n",
    "                       scoring=score,\n",
    "                       cv=kfolds.split(X_train[:50], y_train_cat[:50]))\n",
    "    clf.fit(X_train[:50], y_train_cat[:50])\n",
    "\n",
    "    print(\"Best parameters found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Confusion matrix based on the full evaluation set:\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test_cat[:50], clf.predict(X_test[:50])\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print()\n",
    "    print('Accuracy', np.round(accuracy_score(y_true,y_pred),3))\n",
    "    print('Recall', np.round(recall_score(y_true,y_pred, average='weighted'),3))\n",
    "    print('Precision', np.round(precision_score(y_true,y_pred, average='weighted'),3))\n",
    "    print(\"______________________________________\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {\n",
    "     'max_depth': [3, 7, 10, None],\n",
    "     'n_estimators': [100,200],\n",
    "     'min_samples_split':[2,3,5,7,10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = GridSearchCV(RandomForestRegressor(), \n",
    "                   tuned_parameters,\n",
    "                   cv=3,\n",
    "                   scoring='neg_mean_absolute_error')\n",
    "\n",
    "rg.fit(X_train_rev[:150], y_train_rev[:150])\n",
    "\n",
    "print('Best parameters:', rg.best_params_)\n",
    "print()\n",
    "print('Best score:', rg.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw some final conclusions about this project work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
